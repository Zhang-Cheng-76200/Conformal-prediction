{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhang-Cheng-76200/CapsNet-LSTM/blob/main/CapsNet_LSTM_Gold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G92ON7NXEnkW"
      },
      "source": [
        "# Gold Price Forecasting Using A Hybrid CapsNet-LSTM Architecture\n",
        "\n",
        "Gold dataset contains data of the daily Gold prices recorded from Jan 1st, 2014 to Apr 30st, 2018. In addition to be using Tensorflow's layers for processing sequence data such as LSTMs we will also intergrate Capsule Network in our proposed neural architecture to improve the model's performance.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "56XEQOGknrAk"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras import initializers, layers, models, optimizers, callbacks, utils\n",
        "from keras.models import load_model\n",
        "from keras.models import model_from_json\n",
        "\n",
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "from keras.initializers import *\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.optimizers import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import *\n",
        "\n",
        "from keras import losses\n",
        "from keras.utils.io_utils import ask_to_proceed_with_overwrite\n",
        "from tensorflow.python.platform import tf_logging as logging\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "tf.random.set_seed(1)"
      ],
      "metadata": {
        "id": "Xe-Rnq9QbQ7A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If code is running on TPU\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ],
      "metadata": {
        "id": "CnjuziNxZdvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e490d383-3c4a-49e1-f1db-095c506c1644"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "metadata": {
        "id": "eb5Za_rOMFVx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "rA-JsdF3y5eE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4f12c9-12a2-470b-a0f2-1d2401f35a69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBR7h1IgEnkc"
      },
      "source": [
        "## Parsing the raw data\n",
        "\n",
        "A couple of things to note:\n",
        "\n",
        "- There is no need to save the data points as numpy arrays, regular lists is fine.\n",
        "- The `time` list should contain every timestep (starting at zero), which is just a sequence of ordered numbers with the same length as the `series` list.\n",
        "- The values of the `series` should be of `float` type. You can use Python's built-in `float` function to ensure this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeP3JMZHEnkZ"
      },
      "source": [
        "Begin by looking at the structure of the xlsx file that contains the data:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('gdrive')"
      ],
      "metadata": {
        "id": "o4Vkj4Y8DYm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22f8ddc9-1606-4ea3-ccad-972cbee5ad37"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Gold_raw = pd.read_excel('/content/gdrive/MyDrive/Gold/Gold.xlsx')\n",
        "Gold_raw"
      ],
      "metadata": {
        "id": "vKv3toTHFgnh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "80096089-a16f-4c1b-9b1b-5db533333fd9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Date    Price\n",
              "0    2007-10-01   742.50\n",
              "1    2007-10-02   731.00\n",
              "2    2007-10-03   730.25\n",
              "3    2007-10-04   725.50\n",
              "4    2007-10-05   737.00\n",
              "...         ...      ...\n",
              "3385 2020-09-21  1909.35\n",
              "3386 2020-09-22  1906.00\n",
              "3387 2020-09-23  1873.40\n",
              "3388 2020-09-24  1861.75\n",
              "3389 2020-09-25  1859.70\n",
              "\n",
              "[3390 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-94a056fc-bdc0-49b8-96b9-ef5ea6430f3b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2007-10-01</td>\n",
              "      <td>742.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2007-10-02</td>\n",
              "      <td>731.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2007-10-03</td>\n",
              "      <td>730.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2007-10-04</td>\n",
              "      <td>725.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2007-10-05</td>\n",
              "      <td>737.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3385</th>\n",
              "      <td>2020-09-21</td>\n",
              "      <td>1909.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3386</th>\n",
              "      <td>2020-09-22</td>\n",
              "      <td>1906.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3387</th>\n",
              "      <td>2020-09-23</td>\n",
              "      <td>1873.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3388</th>\n",
              "      <td>2020-09-24</td>\n",
              "      <td>1861.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3389</th>\n",
              "      <td>2020-09-25</td>\n",
              "      <td>1859.70</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3390 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94a056fc-bdc0-49b8-96b9-ef5ea6430f3b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-94a056fc-bdc0-49b8-96b9-ef5ea6430f3b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-94a056fc-bdc0-49b8-96b9-ef5ea6430f3b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "series = Gold_raw.iloc[:,1] # price"
      ],
      "metadata": {
        "id": "vTg-wdf-Vpbi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "time = []\n",
        "for i in range(0, len(series)):\n",
        "  time.append(i)"
      ],
      "metadata": {
        "id": "4P3cPCrbVpVl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A helper function to plot the time series:"
      ],
      "metadata": {
        "id": "XQh2IH8Tl0Le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
        "    plt.plot(time[start:end], series[start:end], format)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)"
      ],
      "metadata": {
        "id": "qMGyMzGelzEW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT1tB8J1Enkd"
      },
      "source": [
        "The next cell will use functions to compute the `time` and `series` and will save these as numpy arrays within the `G` dataclass. This cell will also plot the time series:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R8UYoZlKEnke",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "e43a49ed-11b0-4562-8228-4cf89780b3a5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Save all \"global\" variables within the G class (G stands for global)\n",
        "@dataclass\n",
        "class G:\n",
        "    TIME = np.array(time)\n",
        "    SERIES = np.array(series)\n",
        "    SPLIT_TIME = int(len(series)*0.9)          # data splitting.\n",
        "    WINDOW_SIZE = 250                                # sequence length\n",
        "    BATCH_SIZE = 32                                  # batch size\n",
        "\n",
        "plt.rcParams['font.size'] = '16'\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(G.TIME, G.SERIES)\n",
        "plt.ylabel(\"USD\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "legend_drawn_flag = True\n",
        "plt.legend([\"Gold per ounce price\"], loc=0, frameon=legend_drawn_flag)\n",
        "plt.savefig(\"/content/gdrive/MyDrive/Forecasting_results/Gold_original.svg\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8U3ckcaEnke"
      },
      "source": [
        "## Processing the data\n",
        "\n",
        "The `train_val_split` and `windowed_dataset` functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3O7A_9XdEnkf"
      },
      "outputs": [],
      "source": [
        "def train_test_split(time, series, time_step=G.SPLIT_TIME):\n",
        "\n",
        "    time_train = time[:time_step]\n",
        "    series_train = series[:time_step]\n",
        "    time_test = time[time_step:]\n",
        "    series_test = series[time_step:]\n",
        "   \n",
        "\n",
        "    return time_train, series_train, time_test, series_test\n",
        "\n",
        "# Split the dataset\n",
        "time_train, series_train, time_test, series_test = train_test_split(G.TIME, G.SERIES)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "min-max normalization:"
      ],
      "metadata": {
        "id": "M8xZy3QTZD8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max_normalization(series):\n",
        "  series = (series - min(series_train))/(max(series_train) - min(series_train))\n",
        "  return series"
      ],
      "metadata": {
        "id": "aRkvaM6AauAy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "series_norm = min_max_normalization(G.SERIES)"
      ],
      "metadata": {
        "id": "UR8bYse7ZC-G"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "series_train_norm = series_norm[:G.SPLIT_TIME]\n",
        "series_test_norm = series_norm[G.SPLIT_TIME:]"
      ],
      "metadata": {
        "id": "CQr8LSVw4BIV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building train_set:\n",
        "the train_set is used for training of the optimized model."
      ],
      "metadata": {
        "id": "izPRK1HJS8Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train_set, val_set, and train_val_set. train_set and val_set are used for hyper=parameters tuning, and train_val_set is used for training.\n",
        "def windowed_dataset(series, window_size=G.WINDOW_SIZE, batch_size=G.BATCH_SIZE):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.map(lambda w: (w[:-1], w[-1]))\n",
        "    ds = ds.batch(batch_size).prefetch(1)\n",
        "    return ds\n",
        "train_set = windowed_dataset(series_train_norm, window_size=G.WINDOW_SIZE, batch_size=G.BATCH_SIZE)"
      ],
      "metadata": {
        "id": "Uix6YCADP4wK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06f372c-f194-42b5-fd73-c1947f23b19e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6stIQIqEnkh"
      },
      "source": [
        "## Compiling the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaMon77MEnkf"
      },
      "source": [
        "### Defining the model architecture (CapsNet-LSTM)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Squash function\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True) \n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
        "    return scale * vectors\n"
      ],
      "metadata": {
        "id": "NAPDHkls8FGx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dynamic routing (layer)\n",
        "#    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
        "#    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "#    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
        "#    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
        "\n",
        "#    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
        "#    :param routings: number of iterations for the routing algorithm \n",
        "\n",
        "#@tf.keras.utils.register_keras_serializable() should use this line next time when need to save the model with a custom layer.\n",
        "class Routing(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, dim_capsule = 32 , routings=3, kernel_initializer='glorot_uniform'):\n",
        "        super(Routing, self).__init__()\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # build tansform matrix which can convert one primary cap to a vector with the same order as the digit cap.\n",
        "        # assert len(input_shape) >= 3  # The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "        \n",
        "        # Transform matrix\n",
        "       \n",
        "        self.W = self.add_weight(shape=[self.input_num_capsule,self.dim_capsule, self.input_dim_capsule],initializer=self.kernel_initializer,name='W')\n",
        "\n",
        "    def call(self, inputs): #training=None\n",
        "        # inputs.shape=[input_num_capsule, input_dim_capsule]\n",
        "        # inputs_expand.shape=[input_num_capsule, input_dim_capsule, 1]\n",
        "        inputs_expand = K.expand_dims(inputs, -1)    \n",
        "\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
        "        # x.shape=[None, input_num_capsule, input_dim_capsule, 1]\n",
        "        # W.shape=[input_num_capsule, dim_capsule, input_dim_capsule]\n",
        "        # Regard the first dimension as `batch` dimension,\n",
        "        # then matmul: [dim_capsule, input_dim_capsule] x [input_dim_capsule, 1] -> [dim_capsule].\n",
        "        # inputs_hat.shape = [None, input_num_capsule, dim_capsule, 1]     \n",
        "          \n",
        "        inputs_hat = K.map_fn(lambda x: K.batch_dot(self.W, x, [2, 1]), elems=inputs_expand) \n",
        "\n",
        "        inputs_hat = K.squeeze(inputs_hat, axis = -1)\n",
        "        # inputs_hat.shape = [None, input_num_capsule, dim_capsule]\n",
        "       \n",
        "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        # b.shape = [None, self.input_num_capsule].\n",
        "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.input_num_capsule])\n",
        "        \n",
        "        assert self.routings > 0\n",
        "        for i in range(self.routings):\n",
        "            # c.shape=[batch_size, input_num_capsule]\n",
        "            c = tf.nn.softmax(b, axis=-1)\n",
        "\n",
        "            # c.shape =  [batch_size, input_num_capsule]\n",
        "            # inputs_hat.shape=[batch_size, input_num_capsule, dim_capsule]\n",
        "            # The first dimensions as `batch` dimension,\n",
        "            # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
        "            # outputs.shape=[None, dim_capsule]\n",
        "            \n",
        "            #outputs = squash(K.batch_dot(c, inputs_hat, [1, 1])) \n",
        "            outputs = K.batch_dot(c, inputs_hat, [1, 1])\n",
        "\n",
        "            if i < self.routings - 1:\n",
        "                # outputs.shape =  [None, dim_capsule]\n",
        "                # inputs_hat.shape=[None, input_num_capsule, dim_capsule]\n",
        "                # The first dimension as `batch` dimension,\n",
        "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
        "                # b.shape=[batch_size, input_num_capsule]\n",
        "                b += K.batch_dot(outputs, inputs_hat, [1, 2])\n",
        "       \n",
        "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.dim_capsule])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"dim_capsule\": self.dim_capsule,\n",
        "            \"routings\": self.routings,\n",
        "            \"kernel_initializer\": self.kernel_initializer,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "n2SUkh9LAFqP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1Y12KaJYEnki"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(true_series, forecast):\n",
        "    \n",
        "    mse = tf.keras.metrics.mean_squared_error(true_series, forecast).numpy()   \n",
        "    rmse = tf.math.sqrt(mse).numpy()\n",
        "    mae = tf.keras.metrics.mean_absolute_error(true_series, forecast).numpy()\n",
        "    mape = tf.keras.metrics.mean_absolute_percentage_error(true_series, forecast).numpy()\n",
        "    return rmse, mae, mape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def smape(true_series, forecast):\n",
        "    return 100/len(true_series) * np.sum(2 * np.abs(forecast - true_series) / (np.abs(true_series) + np.abs(forecast)))"
      ],
      "metadata": {
        "id": "JaL1EdZAe1TN"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4XwGrf-A_wF0"
      },
      "outputs": [],
      "source": [
        "def model_forecast(model, series, window_size):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "    ds = ds.batch(32).prefetch(1)\n",
        "    forecast = model.predict(ds)\n",
        "    return forecast"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reverse_normalization(series):\n",
        "  series = series * (max(series_train) - min(series_train)) + min(series_train)\n",
        "  return series"
      ],
      "metadata": {
        "id": "xG8kJO3blWny"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (G.WINDOW_SIZE, 1)"
      ],
      "metadata": {
        "id": "uh3dZo2eVk0K"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare callback\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='mae', factor=0.95,patience=5)"
      ],
      "metadata": {
        "id": "LxTHiYBFhIhF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()"
      ],
      "metadata": {
        "id": "-dD6KzvejX_o"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determine the number of Epoch"
      ],
      "metadata": {
        "id": "HAgEm3Ue4PNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with the default setting. LSTM units = 200, Convolution filters = 512, kernel size = 2, stride = 1, learning rate = 0.0005\n",
        "def CapsNet_LSTM():\n",
        "   \n",
        "    input = tf.keras.layers.Input(shape=input_shape)\n",
        "      \n",
        "    # First layer\n",
        "    conv1d_1 = tf.keras.layers.Conv1D(filters=512, kernel_size=2, strides=1, padding='causal', activation='relu', name = 'conv1d_1')(input)\n",
        "          \n",
        "    # Primary caps\n",
        "  \n",
        "    unsquashed_caps = tf.keras.layers.Reshape((G.WINDOW_SIZE, 64, 8))(conv1d_1)\n",
        "    squashed_caps = tf.keras.layers.Lambda(squash)(unsquashed_caps)\n",
        "\n",
        "    # Digit caps\n",
        "     \n",
        "    digit_caps = tf.keras.layers.TimeDistributed(Routing(dim_capsule = 512, routings = 3))(squashed_caps)\n",
        "\n",
        "    # LSTM layer\n",
        "    lstm = tf.keras.layers.LSTM(100)(digit_caps)\n",
        "\n",
        "    # Dense layer\n",
        "\n",
        "    output = tf.keras.layers.Dense(1)(lstm)\n",
        "\n",
        "    model = keras.Model(inputs=input, outputs=output, name=\"capsnet_lstm\")\n",
        "\n",
        "    learning_rate = 0.0005\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        metrics=[\"mae\"])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "fqOfqDbaUhsm"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding epoch number"
      ],
      "metadata": {
        "id": "Hh0WNIt-XlNR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "epoch number for CapsNet-LSTM is 500."
      ],
      "metadata": {
        "id": "hMTX52gu5Jmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with the train dataset and val set combining together.\n",
        "with strategy.scope():\n",
        "  capsnet_lstm = CapsNet_LSTM()\n",
        "# Fit with the entire dataset.\n",
        "history_capsnet_lstm_train = capsnet_lstm.fit(train_set, epochs=500,callbacks = [reduce_lr])"
      ],
      "metadata": {
        "id": "Bz4kTlU3C_r_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d631b0-95f2-4bff-d042-cc4f5cedde6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "88/88 [==============================] - 43s 409ms/step - loss: 0.0096 - mae: 0.0774 - lr: 5.0000e-04\n",
            "Epoch 2/500\n",
            "88/88 [==============================] - 35s 404ms/step - loss: 0.0419 - mae: 0.1453 - lr: 5.0000e-04\n",
            "Epoch 3/500\n",
            "88/88 [==============================] - 33s 377ms/step - loss: 0.0507 - mae: 0.1767 - lr: 5.0000e-04\n",
            "Epoch 4/500\n",
            "88/88 [==============================] - 38s 432ms/step - loss: 0.0358 - mae: 0.1514 - lr: 5.0000e-04\n",
            "Epoch 5/500\n",
            "88/88 [==============================] - 34s 388ms/step - loss: 0.0271 - mae: 0.1312 - lr: 5.0000e-04\n",
            "Epoch 6/500\n",
            "88/88 [==============================] - 32s 365ms/step - loss: 0.0172 - mae: 0.1016 - lr: 5.0000e-04\n",
            "Epoch 7/500\n",
            "88/88 [==============================] - 35s 405ms/step - loss: 0.0110 - mae: 0.0789 - lr: 4.7500e-04\n",
            "Epoch 8/500\n",
            "88/88 [==============================] - 31s 347ms/step - loss: 0.0071 - mae: 0.0599 - lr: 4.7500e-04\n",
            "Epoch 9/500\n",
            "88/88 [==============================] - 32s 360ms/step - loss: 0.0049 - mae: 0.0473 - lr: 4.7500e-04\n",
            "Epoch 10/500\n",
            "88/88 [==============================] - 35s 389ms/step - loss: 0.0035 - mae: 0.0389 - lr: 4.7500e-04\n",
            "Epoch 11/500\n",
            "88/88 [==============================] - 38s 434ms/step - loss: 0.0027 - mae: 0.0333 - lr: 4.7500e-04\n",
            "Epoch 12/500\n",
            "88/88 [==============================] - 36s 408ms/step - loss: 0.0021 - mae: 0.0295 - lr: 4.7500e-04\n",
            "Epoch 13/500\n",
            "88/88 [==============================] - 32s 367ms/step - loss: 0.0017 - mae: 0.0269 - lr: 4.7500e-04\n",
            "Epoch 14/500\n",
            "88/88 [==============================] - 34s 384ms/step - loss: 0.0015 - mae: 0.0254 - lr: 4.7500e-04\n",
            "Epoch 15/500\n",
            "88/88 [==============================] - 35s 404ms/step - loss: 0.0013 - mae: 0.0246 - lr: 4.7500e-04\n",
            "Epoch 16/500\n",
            "88/88 [==============================] - 35s 401ms/step - loss: 0.0012 - mae: 0.0242 - lr: 4.7500e-04\n",
            "Epoch 17/500\n",
            "88/88 [==============================] - 33s 378ms/step - loss: 0.0012 - mae: 0.0241 - lr: 4.7500e-04\n",
            "Epoch 18/500\n",
            "88/88 [==============================] - 33s 376ms/step - loss: 0.0012 - mae: 0.0241 - lr: 4.7500e-04\n",
            "Epoch 19/500\n",
            "88/88 [==============================] - 37s 415ms/step - loss: 0.0012 - mae: 0.0241 - lr: 4.7500e-04\n",
            "Epoch 20/500\n",
            "88/88 [==============================] - 35s 403ms/step - loss: 0.0012 - mae: 0.0240 - lr: 4.7500e-04\n",
            "Epoch 21/500\n",
            "88/88 [==============================] - 35s 402ms/step - loss: 0.0011 - mae: 0.0239 - lr: 4.7500e-04\n",
            "Epoch 22/500\n",
            "88/88 [==============================] - 34s 391ms/step - loss: 0.0011 - mae: 0.0238 - lr: 4.7500e-04\n",
            "Epoch 23/500\n",
            "88/88 [==============================] - 45s 520ms/step - loss: 0.0011 - mae: 0.0236 - lr: 4.7500e-04\n",
            "Epoch 24/500\n",
            "88/88 [==============================] - 34s 384ms/step - loss: 0.0011 - mae: 0.0235 - lr: 4.7500e-04\n",
            "Epoch 25/500\n",
            "88/88 [==============================] - 37s 422ms/step - loss: 0.0010 - mae: 0.0232 - lr: 4.7500e-04\n",
            "Epoch 26/500\n",
            "88/88 [==============================] - 39s 438ms/step - loss: 0.0010 - mae: 0.0229 - lr: 4.7500e-04\n",
            "Epoch 27/500\n",
            "88/88 [==============================] - 38s 436ms/step - loss: 9.7636e-04 - mae: 0.0226 - lr: 4.7500e-04\n",
            "Epoch 28/500\n",
            "88/88 [==============================] - 33s 372ms/step - loss: 9.4524e-04 - mae: 0.0223 - lr: 4.7500e-04\n",
            "Epoch 29/500\n",
            "88/88 [==============================] - 35s 386ms/step - loss: 9.1798e-04 - mae: 0.0220 - lr: 4.7500e-04\n",
            "Epoch 30/500\n",
            "88/88 [==============================] - 36s 410ms/step - loss: 8.8673e-04 - mae: 0.0216 - lr: 4.7500e-04\n",
            "Epoch 31/500\n",
            "88/88 [==============================] - 33s 376ms/step - loss: 8.5618e-04 - mae: 0.0212 - lr: 4.7500e-04\n",
            "Epoch 32/500\n",
            "88/88 [==============================] - 37s 422ms/step - loss: 8.2820e-04 - mae: 0.0209 - lr: 4.7500e-04\n",
            "Epoch 33/500\n",
            "88/88 [==============================] - 38s 431ms/step - loss: 8.0024e-04 - mae: 0.0206 - lr: 4.7500e-04\n",
            "Epoch 34/500\n",
            "88/88 [==============================] - 36s 398ms/step - loss: 7.7432e-04 - mae: 0.0202 - lr: 4.7500e-04\n",
            "Epoch 35/500\n",
            "88/88 [==============================] - 37s 411ms/step - loss: 7.4516e-04 - mae: 0.0199 - lr: 4.7500e-04\n",
            "Epoch 36/500\n",
            "88/88 [==============================] - 32s 371ms/step - loss: 7.2037e-04 - mae: 0.0195 - lr: 4.7500e-04\n",
            "Epoch 37/500\n",
            "88/88 [==============================] - 30s 340ms/step - loss: 6.9346e-04 - mae: 0.0192 - lr: 4.7500e-04\n",
            "Epoch 38/500\n",
            "88/88 [==============================] - 32s 361ms/step - loss: 6.6872e-04 - mae: 0.0188 - lr: 4.7500e-04\n",
            "Epoch 39/500\n",
            "88/88 [==============================] - 34s 385ms/step - loss: 6.4217e-04 - mae: 0.0185 - lr: 4.7500e-04\n",
            "Epoch 40/500\n",
            "88/88 [==============================] - 35s 389ms/step - loss: 6.1799e-04 - mae: 0.0181 - lr: 4.7500e-04\n",
            "Epoch 41/500\n",
            "88/88 [==============================] - 41s 460ms/step - loss: 5.9324e-04 - mae: 0.0178 - lr: 4.7500e-04\n",
            "Epoch 42/500\n",
            "88/88 [==============================] - 35s 402ms/step - loss: 5.6852e-04 - mae: 0.0174 - lr: 4.7500e-04\n",
            "Epoch 43/500\n",
            "88/88 [==============================] - 35s 399ms/step - loss: 5.4387e-04 - mae: 0.0171 - lr: 4.7500e-04\n",
            "Epoch 44/500\n",
            "88/88 [==============================] - 32s 366ms/step - loss: 5.2007e-04 - mae: 0.0167 - lr: 4.7500e-04\n",
            "Epoch 45/500\n",
            "88/88 [==============================] - 29s 333ms/step - loss: 4.9646e-04 - mae: 0.0163 - lr: 4.7500e-04\n",
            "Epoch 46/500\n",
            "88/88 [==============================] - 29s 329ms/step - loss: 4.7251e-04 - mae: 0.0160 - lr: 4.7500e-04\n",
            "Epoch 47/500\n",
            "88/88 [==============================] - 32s 365ms/step - loss: 4.4940e-04 - mae: 0.0156 - lr: 4.7500e-04\n",
            "Epoch 48/500\n",
            "88/88 [==============================] - 30s 348ms/step - loss: 4.2669e-04 - mae: 0.0152 - lr: 4.7500e-04\n",
            "Epoch 49/500\n",
            "88/88 [==============================] - 35s 401ms/step - loss: 4.0518e-04 - mae: 0.0148 - lr: 4.7500e-04\n",
            "Epoch 50/500\n",
            "88/88 [==============================] - 32s 364ms/step - loss: 3.8522e-04 - mae: 0.0144 - lr: 4.7500e-04\n",
            "Epoch 51/500\n",
            "88/88 [==============================] - 36s 400ms/step - loss: 3.6543e-04 - mae: 0.0141 - lr: 4.7500e-04\n",
            "Epoch 52/500\n",
            "88/88 [==============================] - 34s 379ms/step - loss: 3.4583e-04 - mae: 0.0137 - lr: 4.7500e-04\n",
            "Epoch 53/500\n",
            "88/88 [==============================] - 30s 344ms/step - loss: 3.2900e-04 - mae: 0.0133 - lr: 4.7500e-04\n",
            "Epoch 54/500\n",
            "88/88 [==============================] - 31s 350ms/step - loss: 3.1393e-04 - mae: 0.0130 - lr: 4.7500e-04\n",
            "Epoch 55/500\n",
            "88/88 [==============================] - 33s 373ms/step - loss: 3.0078e-04 - mae: 0.0127 - lr: 4.7500e-04\n",
            "Epoch 56/500\n",
            "88/88 [==============================] - 37s 424ms/step - loss: 2.9030e-04 - mae: 0.0124 - lr: 4.7500e-04\n",
            "Epoch 57/500\n",
            "88/88 [==============================] - 34s 390ms/step - loss: 2.8092e-04 - mae: 0.0122 - lr: 4.7500e-04\n",
            "Epoch 58/500\n",
            "88/88 [==============================] - 35s 393ms/step - loss: 2.7268e-04 - mae: 0.0120 - lr: 4.7500e-04\n",
            "Epoch 59/500\n",
            "88/88 [==============================] - 29s 329ms/step - loss: 2.6675e-04 - mae: 0.0119 - lr: 4.7500e-04\n",
            "Epoch 60/500\n",
            "88/88 [==============================] - 32s 366ms/step - loss: 2.6232e-04 - mae: 0.0117 - lr: 4.7500e-04\n",
            "Epoch 61/500\n",
            "88/88 [==============================] - 35s 400ms/step - loss: 2.5903e-04 - mae: 0.0117 - lr: 4.7500e-04\n",
            "Epoch 62/500\n",
            "88/88 [==============================] - 30s 341ms/step - loss: 2.5798e-04 - mae: 0.0116 - lr: 4.7500e-04\n",
            "Epoch 63/500\n",
            "88/88 [==============================] - 36s 409ms/step - loss: 2.5691e-04 - mae: 0.0116 - lr: 4.7500e-04\n",
            "Epoch 64/500\n",
            "88/88 [==============================] - 35s 388ms/step - loss: 2.5613e-04 - mae: 0.0116 - lr: 4.7500e-04\n",
            "Epoch 65/500\n",
            "88/88 [==============================] - 29s 328ms/step - loss: 2.5563e-04 - mae: 0.0116 - lr: 4.7500e-04\n",
            "Epoch 66/500\n",
            "88/88 [==============================] - 33s 383ms/step - loss: 2.5679e-04 - mae: 0.0116 - lr: 4.7500e-04\n",
            "Epoch 67/500\n",
            "88/88 [==============================] - 34s 383ms/step - loss: 2.5865e-04 - mae: 0.0116 - lr: 4.7500e-04\n",
            "Epoch 68/500\n",
            "88/88 [==============================] - 31s 360ms/step - loss: 2.5671e-04 - mae: 0.0116 - lr: 4.5125e-04\n",
            "Epoch 69/500\n",
            "88/88 [==============================] - 31s 341ms/step - loss: 2.5864e-04 - mae: 0.0116 - lr: 4.5125e-04\n",
            "Epoch 70/500\n",
            "88/88 [==============================] - 36s 406ms/step - loss: 2.6195e-04 - mae: 0.0117 - lr: 4.5125e-04\n",
            "Epoch 71/500\n",
            "88/88 [==============================] - 34s 394ms/step - loss: 2.6424e-04 - mae: 0.0118 - lr: 4.5125e-04\n",
            "Epoch 72/500\n",
            "88/88 [==============================] - 31s 358ms/step - loss: 2.6775e-04 - mae: 0.0119 - lr: 4.5125e-04\n",
            "Epoch 73/500\n",
            "88/88 [==============================] - 36s 408ms/step - loss: 2.6750e-04 - mae: 0.0119 - lr: 4.2869e-04\n",
            "Epoch 74/500\n",
            "88/88 [==============================] - 32s 361ms/step - loss: 2.6962e-04 - mae: 0.0119 - lr: 4.2869e-04\n",
            "Epoch 75/500\n",
            "88/88 [==============================] - 40s 451ms/step - loss: 2.7068e-04 - mae: 0.0119 - lr: 4.2869e-04\n",
            "Epoch 76/500\n",
            "88/88 [==============================] - 36s 416ms/step - loss: 2.7295e-04 - mae: 0.0120 - lr: 4.2869e-04\n",
            "Epoch 77/500\n",
            "88/88 [==============================] - 31s 349ms/step - loss: 2.7458e-04 - mae: 0.0120 - lr: 4.2869e-04\n",
            "Epoch 78/500\n",
            "88/88 [==============================] - 32s 365ms/step - loss: 2.7058e-04 - mae: 0.0119 - lr: 4.0725e-04\n",
            "Epoch 79/500\n",
            "88/88 [==============================] - 32s 365ms/step - loss: 2.7097e-04 - mae: 0.0119 - lr: 4.0725e-04\n",
            "Epoch 80/500\n",
            "88/88 [==============================] - 31s 355ms/step - loss: 2.6797e-04 - mae: 0.0118 - lr: 4.0725e-04\n",
            "Epoch 81/500\n",
            "88/88 [==============================] - 34s 387ms/step - loss: 2.6828e-04 - mae: 0.0118 - lr: 4.0725e-04\n",
            "Epoch 82/500\n",
            "88/88 [==============================] - 36s 408ms/step - loss: 2.6836e-04 - mae: 0.0118 - lr: 4.0725e-04\n",
            "Epoch 83/500\n",
            "88/88 [==============================] - 38s 431ms/step - loss: 2.6128e-04 - mae: 0.0117 - lr: 3.8689e-04\n",
            "Epoch 84/500\n",
            "88/88 [==============================] - 37s 428ms/step - loss: 2.5941e-04 - mae: 0.0116 - lr: 3.8689e-04\n",
            "Epoch 85/500\n",
            "88/88 [==============================] - 37s 418ms/step - loss: 2.5766e-04 - mae: 0.0116 - lr: 3.8689e-04\n",
            "Epoch 86/500\n",
            "88/88 [==============================] - 35s 399ms/step - loss: 2.5504e-04 - mae: 0.0115 - lr: 3.8689e-04\n",
            "Epoch 87/500\n",
            "88/88 [==============================] - 33s 379ms/step - loss: 2.5516e-04 - mae: 0.0115 - lr: 3.8689e-04\n",
            "Epoch 88/500\n",
            "88/88 [==============================] - 35s 406ms/step - loss: 2.5023e-04 - mae: 0.0114 - lr: 3.6755e-04\n",
            "Epoch 89/500\n",
            "88/88 [==============================] - 33s 379ms/step - loss: 2.4841e-04 - mae: 0.0114 - lr: 3.6755e-04\n",
            "Epoch 90/500\n",
            "88/88 [==============================] - 28s 310ms/step - loss: 2.4641e-04 - mae: 0.0113 - lr: 3.6755e-04\n",
            "Epoch 91/500\n",
            "88/88 [==============================] - 32s 368ms/step - loss: 2.4749e-04 - mae: 0.0114 - lr: 3.6755e-04\n",
            "Epoch 92/500\n",
            "88/88 [==============================] - 33s 375ms/step - loss: 2.4641e-04 - mae: 0.0113 - lr: 3.6755e-04\n",
            "Epoch 93/500\n",
            "88/88 [==============================] - 36s 413ms/step - loss: 2.4614e-04 - mae: 0.0113 - lr: 3.6755e-04\n",
            "Epoch 94/500\n",
            "88/88 [==============================] - 38s 429ms/step - loss: 2.4287e-04 - mae: 0.0112 - lr: 3.4917e-04\n",
            "Epoch 95/500\n",
            "88/88 [==============================] - 30s 338ms/step - loss: 2.4007e-04 - mae: 0.0112 - lr: 3.4917e-04\n",
            "Epoch 96/500\n",
            "88/88 [==============================] - 31s 354ms/step - loss: 2.3810e-04 - mae: 0.0111 - lr: 3.4917e-04\n",
            "Epoch 97/500\n",
            "88/88 [==============================] - 34s 390ms/step - loss: 2.3844e-04 - mae: 0.0111 - lr: 3.4917e-04\n",
            "Epoch 98/500\n",
            "88/88 [==============================] - 33s 372ms/step - loss: 2.3644e-04 - mae: 0.0111 - lr: 3.4917e-04\n",
            "Epoch 99/500\n",
            "88/88 [==============================] - 31s 351ms/step - loss: 2.3800e-04 - mae: 0.0111 - lr: 3.4917e-04\n",
            "Epoch 100/500\n",
            "88/88 [==============================] - 31s 355ms/step - loss: 2.3866e-04 - mae: 0.0111 - lr: 3.4917e-04\n",
            "Epoch 101/500\n",
            "88/88 [==============================] - 29s 328ms/step - loss: 2.3674e-04 - mae: 0.0111 - lr: 3.4917e-04\n",
            "Epoch 102/500\n",
            "88/88 [==============================] - 30s 335ms/step - loss: 2.3097e-04 - mae: 0.0110 - lr: 3.3171e-04\n",
            "Epoch 103/500\n",
            "88/88 [==============================] - 30s 344ms/step - loss: 2.2991e-04 - mae: 0.0109 - lr: 3.3171e-04\n",
            "Epoch 104/500\n",
            "88/88 [==============================] - 33s 372ms/step - loss: 2.2659e-04 - mae: 0.0108 - lr: 3.3171e-04\n",
            "Epoch 105/500\n",
            "88/88 [==============================] - 35s 404ms/step - loss: 2.2436e-04 - mae: 0.0108 - lr: 3.3171e-04\n",
            "Epoch 106/500\n",
            "88/88 [==============================] - 37s 419ms/step - loss: 2.2567e-04 - mae: 0.0108 - lr: 3.3171e-04\n",
            "Epoch 107/500\n",
            "88/88 [==============================] - 37s 420ms/step - loss: 2.2523e-04 - mae: 0.0108 - lr: 3.3171e-04\n",
            "Epoch 108/500\n",
            "88/88 [==============================] - 34s 381ms/step - loss: 2.2552e-04 - mae: 0.0108 - lr: 3.3171e-04\n",
            "Epoch 109/500\n",
            "88/88 [==============================] - 37s 418ms/step - loss: 2.2507e-04 - mae: 0.0108 - lr: 3.3171e-04\n",
            "Epoch 110/500\n",
            "88/88 [==============================] - 37s 412ms/step - loss: 2.2126e-04 - mae: 0.0107 - lr: 3.1512e-04\n",
            "Epoch 111/500\n",
            "88/88 [==============================] - 32s 362ms/step - loss: 2.1952e-04 - mae: 0.0106 - lr: 3.1512e-04\n",
            "Epoch 112/500\n",
            "88/88 [==============================] - 36s 405ms/step - loss: 2.1692e-04 - mae: 0.0106 - lr: 3.1512e-04\n",
            "Epoch 113/500\n",
            "88/88 [==============================] - 30s 340ms/step - loss: 2.1725e-04 - mae: 0.0106 - lr: 3.1512e-04\n",
            "Epoch 114/500\n",
            "88/88 [==============================] - 37s 414ms/step - loss: 2.1555e-04 - mae: 0.0105 - lr: 3.1512e-04\n",
            "Epoch 115/500\n",
            "88/88 [==============================] - 36s 411ms/step - loss: 2.1488e-04 - mae: 0.0105 - lr: 3.1512e-04\n",
            "Epoch 116/500\n",
            "88/88 [==============================] - 36s 411ms/step - loss: 2.1512e-04 - mae: 0.0105 - lr: 3.1512e-04\n",
            "Epoch 117/500\n",
            "88/88 [==============================] - 34s 390ms/step - loss: 2.1592e-04 - mae: 0.0105 - lr: 3.1512e-04\n",
            "Epoch 118/500\n",
            "88/88 [==============================] - 32s 359ms/step - loss: 2.1061e-04 - mae: 0.0104 - lr: 2.9937e-04\n",
            "Epoch 119/500\n",
            "88/88 [==============================] - 34s 389ms/step - loss: 2.0872e-04 - mae: 0.0103 - lr: 2.9937e-04\n",
            "Epoch 120/500\n",
            "88/88 [==============================] - 35s 396ms/step - loss: 2.0785e-04 - mae: 0.0103 - lr: 2.9937e-04\n",
            "Epoch 121/500\n",
            "88/88 [==============================] - 32s 367ms/step - loss: 2.0719e-04 - mae: 0.0103 - lr: 2.9937e-04\n",
            "Epoch 122/500\n",
            "88/88 [==============================] - 33s 378ms/step - loss: 2.0543e-04 - mae: 0.0102 - lr: 2.9937e-04\n",
            "Epoch 123/500\n",
            "88/88 [==============================] - 31s 348ms/step - loss: 2.0548e-04 - mae: 0.0102 - lr: 2.9937e-04\n",
            "Epoch 124/500\n",
            "88/88 [==============================] - 34s 385ms/step - loss: 2.0705e-04 - mae: 0.0103 - lr: 2.9937e-04\n",
            "Epoch 125/500\n",
            "88/88 [==============================] - 34s 374ms/step - loss: 2.0587e-04 - mae: 0.0103 - lr: 2.9937e-04\n",
            "Epoch 126/500\n",
            "88/88 [==============================] - 33s 372ms/step - loss: 2.0525e-04 - mae: 0.0102 - lr: 2.9937e-04\n",
            "Epoch 127/500\n",
            "88/88 [==============================] - 33s 379ms/step - loss: 2.0112e-04 - mae: 0.0101 - lr: 2.8440e-04\n",
            "Epoch 128/500\n",
            "88/88 [==============================] - 31s 352ms/step - loss: 2.0069e-04 - mae: 0.0101 - lr: 2.8440e-04\n",
            "Epoch 129/500\n",
            "88/88 [==============================] - 33s 372ms/step - loss: 1.9919e-04 - mae: 0.0101 - lr: 2.8440e-04\n",
            "Epoch 130/500\n",
            "88/88 [==============================] - 31s 349ms/step - loss: 1.9692e-04 - mae: 0.0100 - lr: 2.8440e-04\n",
            "Epoch 131/500\n",
            "88/88 [==============================] - 34s 390ms/step - loss: 1.9715e-04 - mae: 0.0100 - lr: 2.8440e-04\n",
            "Epoch 132/500\n",
            "88/88 [==============================] - 33s 375ms/step - loss: 1.9640e-04 - mae: 0.0100 - lr: 2.8440e-04\n",
            "Epoch 133/500\n",
            "88/88 [==============================] - 36s 411ms/step - loss: 1.9702e-04 - mae: 0.0100 - lr: 2.8440e-04\n",
            "Epoch 134/500\n",
            "88/88 [==============================] - 38s 427ms/step - loss: 1.9564e-04 - mae: 0.0100 - lr: 2.8440e-04\n",
            "Epoch 135/500\n",
            "88/88 [==============================] - 32s 371ms/step - loss: 1.9676e-04 - mae: 0.0100 - lr: 2.8440e-04\n",
            "Epoch 136/500\n",
            "88/88 [==============================] - 32s 361ms/step - loss: 1.9324e-04 - mae: 0.0099 - lr: 2.7018e-04\n",
            "Epoch 137/500\n",
            "88/88 [==============================] - 37s 420ms/step - loss: 1.9355e-04 - mae: 0.0099 - lr: 2.7018e-04\n",
            "Epoch 138/500\n",
            "88/88 [==============================] - 34s 379ms/step - loss: 1.9146e-04 - mae: 0.0098 - lr: 2.7018e-04\n",
            "Epoch 139/500\n",
            "88/88 [==============================] - 36s 404ms/step - loss: 1.9071e-04 - mae: 0.0098 - lr: 2.7018e-04\n",
            "Epoch 140/500\n",
            "88/88 [==============================] - 34s 390ms/step - loss: 1.9023e-04 - mae: 0.0098 - lr: 2.7018e-04\n",
            "Epoch 141/500\n",
            "88/88 [==============================] - 31s 357ms/step - loss: 1.8993e-04 - mae: 0.0098 - lr: 2.7018e-04\n",
            "Epoch 142/500\n",
            "88/88 [==============================] - 32s 361ms/step - loss: 1.8461e-04 - mae: 0.0096 - lr: 2.5667e-04\n",
            "Epoch 143/500\n",
            "88/88 [==============================] - 30s 337ms/step - loss: 1.8566e-04 - mae: 0.0097 - lr: 2.5667e-04\n",
            "Epoch 144/500\n",
            "88/88 [==============================] - 35s 404ms/step - loss: 1.8499e-04 - mae: 0.0096 - lr: 2.5667e-04\n",
            "Epoch 145/500\n",
            "88/88 [==============================] - 34s 392ms/step - loss: 1.8365e-04 - mae: 0.0096 - lr: 2.5667e-04\n",
            "Epoch 146/500\n",
            "88/88 [==============================] - 35s 389ms/step - loss: 1.8428e-04 - mae: 0.0096 - lr: 2.5667e-04\n",
            "Epoch 147/500\n",
            "88/88 [==============================] - 35s 394ms/step - loss: 1.8254e-04 - mae: 0.0096 - lr: 2.5667e-04\n",
            "Epoch 148/500\n",
            "88/88 [==============================] - 37s 423ms/step - loss: 1.8079e-04 - mae: 0.0095 - lr: 2.4384e-04\n",
            "Epoch 149/500\n",
            "88/88 [==============================] - 34s 392ms/step - loss: 1.8225e-04 - mae: 0.0096 - lr: 2.4384e-04\n",
            "Epoch 150/500\n",
            "88/88 [==============================] - 31s 349ms/step - loss: 1.8036e-04 - mae: 0.0095 - lr: 2.4384e-04\n",
            "Epoch 151/500\n",
            "88/88 [==============================] - 32s 364ms/step - loss: 1.8014e-04 - mae: 0.0095 - lr: 2.4384e-04\n",
            "Epoch 152/500\n",
            "88/88 [==============================] - 36s 406ms/step - loss: 1.7926e-04 - mae: 0.0095 - lr: 2.4384e-04\n",
            "Epoch 153/500\n",
            "88/88 [==============================] - 32s 371ms/step - loss: 1.7910e-04 - mae: 0.0095 - lr: 2.4384e-04\n",
            "Epoch 154/500\n",
            "88/88 [==============================] - 34s 383ms/step - loss: 1.7653e-04 - mae: 0.0094 - lr: 2.3165e-04\n",
            "Epoch 155/500\n",
            "88/88 [==============================] - 31s 352ms/step - loss: 1.7690e-04 - mae: 0.0094 - lr: 2.3165e-04\n",
            "Epoch 156/500\n",
            "88/88 [==============================] - 29s 330ms/step - loss: 1.7564e-04 - mae: 0.0093 - lr: 2.3165e-04\n",
            "Epoch 157/500\n",
            "88/88 [==============================] - 36s 400ms/step - loss: 1.7429e-04 - mae: 0.0093 - lr: 2.3165e-04\n",
            "Epoch 158/500\n",
            "88/88 [==============================] - 32s 365ms/step - loss: 1.7498e-04 - mae: 0.0093 - lr: 2.3165e-04\n",
            "Epoch 159/500\n",
            "88/88 [==============================] - 37s 414ms/step - loss: 1.7450e-04 - mae: 0.0093 - lr: 2.3165e-04\n",
            "Epoch 160/500\n",
            "88/88 [==============================] - 37s 417ms/step - loss: 1.7197e-04 - mae: 0.0092 - lr: 2.2006e-04\n",
            "Epoch 161/500\n",
            "88/88 [==============================] - 37s 421ms/step - loss: 1.7268e-04 - mae: 0.0092 - lr: 2.2006e-04\n",
            "Epoch 162/500\n",
            "88/88 [==============================] - 35s 399ms/step - loss: 1.7227e-04 - mae: 0.0092 - lr: 2.2006e-04\n",
            "Epoch 163/500\n",
            "88/88 [==============================] - 36s 410ms/step - loss: 1.7165e-04 - mae: 0.0092 - lr: 2.2006e-04\n",
            "Epoch 164/500\n",
            "88/88 [==============================] - 33s 376ms/step - loss: 1.7128e-04 - mae: 0.0092 - lr: 2.2006e-04\n",
            "Epoch 165/500\n",
            "88/88 [==============================] - 36s 417ms/step - loss: 1.7135e-04 - mae: 0.0092 - lr: 2.2006e-04\n",
            "Epoch 166/500\n",
            "88/88 [==============================] - 35s 404ms/step - loss: 1.6869e-04 - mae: 0.0091 - lr: 2.0906e-04\n",
            "Epoch 167/500\n",
            "88/88 [==============================] - 35s 402ms/step - loss: 1.6924e-04 - mae: 0.0091 - lr: 2.0906e-04\n",
            "Epoch 168/500\n",
            "88/88 [==============================] - 35s 401ms/step - loss: 1.6862e-04 - mae: 0.0091 - lr: 2.0906e-04\n",
            "Epoch 169/500\n",
            "88/88 [==============================] - 36s 404ms/step - loss: 1.6830e-04 - mae: 0.0091 - lr: 2.0906e-04\n",
            "Epoch 170/500\n",
            "88/88 [==============================] - 35s 395ms/step - loss: 1.6861e-04 - mae: 0.0091 - lr: 2.0906e-04\n",
            "Epoch 171/500\n",
            "88/88 [==============================] - 32s 368ms/step - loss: 1.6820e-04 - mae: 0.0091 - lr: 2.0906e-04\n",
            "Epoch 172/500\n",
            "88/88 [==============================] - 35s 401ms/step - loss: 1.6620e-04 - mae: 0.0090 - lr: 1.9861e-04\n",
            "Epoch 173/500\n",
            "88/88 [==============================] - 36s 401ms/step - loss: 1.6603e-04 - mae: 0.0090 - lr: 1.9861e-04\n",
            "Epoch 174/500\n",
            "88/88 [==============================] - 37s 428ms/step - loss: 1.6646e-04 - mae: 0.0090 - lr: 1.9861e-04\n",
            "Epoch 175/500\n",
            "88/88 [==============================] - 31s 354ms/step - loss: 1.6738e-04 - mae: 0.0091 - lr: 1.9861e-04\n",
            "Epoch 176/500\n",
            "88/88 [==============================] - 36s 405ms/step - loss: 1.6635e-04 - mae: 0.0090 - lr: 1.9861e-04\n",
            "Epoch 177/500\n",
            "88/88 [==============================] - 35s 402ms/step - loss: 1.6402e-04 - mae: 0.0090 - lr: 1.8868e-04\n",
            "Epoch 178/500\n",
            "88/88 [==============================] - 29s 332ms/step - loss: 1.6499e-04 - mae: 0.0090 - lr: 1.8868e-04\n",
            "Epoch 179/500\n",
            "88/88 [==============================] - 32s 359ms/step - loss: 1.6544e-04 - mae: 0.0090 - lr: 1.8868e-04\n",
            "Epoch 180/500\n",
            "88/88 [==============================] - 29s 334ms/step - loss: 1.6535e-04 - mae: 0.0090 - lr: 1.8868e-04\n",
            "Epoch 181/500\n",
            "88/88 [==============================] - 30s 340ms/step - loss: 1.6508e-04 - mae: 0.0090 - lr: 1.8868e-04\n",
            "Epoch 182/500\n",
            "88/88 [==============================] - 31s 343ms/step - loss: 1.6549e-04 - mae: 0.0090 - lr: 1.8868e-04\n",
            "Epoch 183/500\n",
            "88/88 [==============================] - 34s 383ms/step - loss: 1.6392e-04 - mae: 0.0090 - lr: 1.7924e-04\n",
            "Epoch 184/500\n",
            "88/88 [==============================] - 32s 371ms/step - loss: 1.6402e-04 - mae: 0.0090 - lr: 1.7924e-04\n",
            "Epoch 185/500\n",
            "88/88 [==============================] - 32s 366ms/step - loss: 1.6485e-04 - mae: 0.0090 - lr: 1.7924e-04\n",
            "Epoch 186/500\n",
            "88/88 [==============================] - 28s 313ms/step - loss: 1.6616e-04 - mae: 0.0090 - lr: 1.7924e-04\n",
            "Epoch 187/500\n",
            "88/88 [==============================] - 33s 382ms/step - loss: 1.6639e-04 - mae: 0.0090 - lr: 1.7924e-04\n",
            "Epoch 188/500\n",
            "88/88 [==============================] - 31s 352ms/step - loss: 1.6499e-04 - mae: 0.0090 - lr: 1.7028e-04\n",
            "Epoch 189/500\n",
            "88/88 [==============================] - 36s 401ms/step - loss: 1.6571e-04 - mae: 0.0090 - lr: 1.7028e-04\n",
            "Epoch 190/500\n",
            "88/88 [==============================] - 33s 379ms/step - loss: 1.6691e-04 - mae: 0.0090 - lr: 1.7028e-04\n",
            "Epoch 191/500\n",
            "88/88 [==============================] - 33s 380ms/step - loss: 1.6782e-04 - mae: 0.0091 - lr: 1.7028e-04\n",
            "Epoch 192/500\n",
            "88/88 [==============================] - 33s 374ms/step - loss: 1.6740e-04 - mae: 0.0091 - lr: 1.7028e-04\n",
            "Epoch 193/500\n",
            "88/88 [==============================] - 35s 391ms/step - loss: 1.6606e-04 - mae: 0.0090 - lr: 1.6177e-04\n",
            "Epoch 194/500\n",
            "88/88 [==============================] - 37s 414ms/step - loss: 1.6738e-04 - mae: 0.0090 - lr: 1.6177e-04\n",
            "Epoch 195/500\n",
            "88/88 [==============================] - 31s 344ms/step - loss: 1.6902e-04 - mae: 0.0091 - lr: 1.6177e-04\n",
            "Epoch 196/500\n",
            "88/88 [==============================] - 32s 365ms/step - loss: 1.7121e-04 - mae: 0.0092 - lr: 1.6177e-04\n",
            "Epoch 197/500\n",
            "88/88 [==============================] - 37s 420ms/step - loss: 1.7218e-04 - mae: 0.0092 - lr: 1.6177e-04\n",
            "Epoch 198/500\n",
            "88/88 [==============================] - 31s 354ms/step - loss: 1.7381e-04 - mae: 0.0092 - lr: 1.5368e-04\n",
            "Epoch 199/500\n",
            "88/88 [==============================] - 36s 411ms/step - loss: 1.7658e-04 - mae: 0.0093 - lr: 1.5368e-04\n",
            "Epoch 200/500\n",
            "88/88 [==============================] - 33s 380ms/step - loss: 1.8117e-04 - mae: 0.0095 - lr: 1.5368e-04\n",
            "Epoch 201/500\n",
            "88/88 [==============================] - 34s 378ms/step - loss: 1.8365e-04 - mae: 0.0095 - lr: 1.5368e-04\n",
            "Epoch 202/500\n",
            "88/88 [==============================] - 35s 402ms/step - loss: 1.8694e-04 - mae: 0.0096 - lr: 1.5368e-04\n",
            "Epoch 203/500\n",
            "88/88 [==============================] - 35s 396ms/step - loss: 1.9600e-04 - mae: 0.0098 - lr: 1.4599e-04\n",
            "Epoch 204/500\n",
            "88/88 [==============================] - 33s 375ms/step - loss: 2.0514e-04 - mae: 0.0101 - lr: 1.4599e-04\n",
            "Epoch 205/500\n",
            "88/88 [==============================] - 33s 370ms/step - loss: 2.2395e-04 - mae: 0.0105 - lr: 1.4599e-04\n",
            "Epoch 206/500\n",
            "88/88 [==============================] - 33s 376ms/step - loss: 2.4896e-04 - mae: 0.0111 - lr: 1.4599e-04\n",
            "Epoch 207/500\n",
            "88/88 [==============================] - 33s 371ms/step - loss: 2.8098e-04 - mae: 0.0118 - lr: 1.4599e-04\n",
            "Epoch 208/500\n",
            "88/88 [==============================] - 39s 447ms/step - loss: 3.3524e-04 - mae: 0.0130 - lr: 1.3869e-04\n",
            "Epoch 209/500\n",
            "88/88 [==============================] - 31s 347ms/step - loss: 3.8619e-04 - mae: 0.0141 - lr: 1.3869e-04\n",
            "Epoch 210/500\n",
            "88/88 [==============================] - 32s 365ms/step - loss: 4.7960e-04 - mae: 0.0157 - lr: 1.3869e-04\n",
            "Epoch 211/500\n",
            "88/88 [==============================] - 37s 417ms/step - loss: 5.0464e-04 - mae: 0.0162 - lr: 1.3869e-04\n",
            "Epoch 212/500\n",
            "88/88 [==============================] - 33s 377ms/step - loss: 6.8644e-04 - mae: 0.0189 - lr: 1.3869e-04\n",
            "Epoch 213/500\n",
            "88/88 [==============================] - 34s 385ms/step - loss: 6.4381e-04 - mae: 0.0183 - lr: 1.3176e-04\n",
            "Epoch 214/500\n",
            "88/88 [==============================] - 32s 361ms/step - loss: 7.2004e-04 - mae: 0.0193 - lr: 1.3176e-04\n",
            "Epoch 215/500\n",
            "88/88 [==============================] - 31s 353ms/step - loss: 5.4474e-04 - mae: 0.0167 - lr: 1.3176e-04\n",
            "Epoch 216/500\n",
            "88/88 [==============================] - 37s 418ms/step - loss: 4.7237e-04 - mae: 0.0156 - lr: 1.3176e-04\n",
            "Epoch 217/500\n",
            "88/88 [==============================] - 35s 399ms/step - loss: 3.7772e-04 - mae: 0.0140 - lr: 1.3176e-04\n",
            "Epoch 218/500\n",
            "88/88 [==============================] - 36s 414ms/step - loss: 3.2055e-04 - mae: 0.0128 - lr: 1.2517e-04\n",
            "Epoch 219/500\n",
            "88/88 [==============================] - 38s 428ms/step - loss: 2.6931e-04 - mae: 0.0117 - lr: 1.2517e-04\n",
            "Epoch 220/500\n",
            "88/88 [==============================] - 35s 403ms/step - loss: 2.3805e-04 - mae: 0.0110 - lr: 1.2517e-04\n",
            "Epoch 221/500\n",
            "88/88 [==============================] - 32s 359ms/step - loss: 2.2081e-04 - mae: 0.0105 - lr: 1.2517e-04\n",
            "Epoch 222/500\n",
            "88/88 [==============================] - 31s 355ms/step - loss: 2.1055e-04 - mae: 0.0103 - lr: 1.2517e-04\n",
            "Epoch 223/500\n",
            "88/88 [==============================] - 33s 376ms/step - loss: 2.0799e-04 - mae: 0.0102 - lr: 1.1891e-04\n",
            "Epoch 224/500\n",
            "88/88 [==============================] - 32s 365ms/step - loss: 2.0118e-04 - mae: 0.0100 - lr: 1.1891e-04\n",
            "Epoch 225/500\n",
            "88/88 [==============================] - 33s 376ms/step - loss: 1.9771e-04 - mae: 0.0099 - lr: 1.1891e-04\n",
            "Epoch 226/500\n",
            "88/88 [==============================] - 32s 365ms/step - loss: 1.9674e-04 - mae: 0.0099 - lr: 1.1891e-04\n",
            "Epoch 227/500\n",
            "88/88 [==============================] - 34s 384ms/step - loss: 1.9856e-04 - mae: 0.0100 - lr: 1.1891e-04\n",
            "Epoch 228/500\n",
            "88/88 [==============================] - 30s 338ms/step - loss: 2.0474e-04 - mae: 0.0101 - lr: 1.1297e-04\n",
            "Epoch 229/500\n",
            "88/88 [==============================] - 33s 365ms/step - loss: 2.0433e-04 - mae: 0.0101 - lr: 1.1297e-04\n",
            "Epoch 230/500\n",
            "88/88 [==============================] - 34s 392ms/step - loss: 2.0620e-04 - mae: 0.0102 - lr: 1.1297e-04\n",
            "Epoch 231/500\n",
            "88/88 [==============================] - 37s 422ms/step - loss: 2.0950e-04 - mae: 0.0103 - lr: 1.1297e-04\n",
            "Epoch 232/500\n",
            "88/88 [==============================] - 31s 355ms/step - loss: 2.1382e-04 - mae: 0.0104 - lr: 1.1297e-04\n",
            "Epoch 233/500\n",
            "88/88 [==============================] - 33s 382ms/step - loss: 2.2368e-04 - mae: 0.0106 - lr: 1.0732e-04\n",
            "Epoch 234/500\n",
            "88/88 [==============================] - 34s 387ms/step - loss: 2.2286e-04 - mae: 0.0106 - lr: 1.0732e-04\n",
            "Epoch 235/500\n",
            "88/88 [==============================] - 32s 361ms/step - loss: 2.2485e-04 - mae: 0.0107 - lr: 1.0732e-04\n",
            "Epoch 236/500\n",
            "88/88 [==============================] - 36s 411ms/step - loss: 2.2747e-04 - mae: 0.0107 - lr: 1.0732e-04\n",
            "Epoch 237/500\n",
            "88/88 [==============================] - 32s 361ms/step - loss: 2.3126e-04 - mae: 0.0108 - lr: 1.0732e-04\n",
            "Epoch 238/500\n",
            "88/88 [==============================] - 37s 421ms/step - loss: 2.3935e-04 - mae: 0.0110 - lr: 1.0195e-04\n",
            "Epoch 239/500\n",
            "88/88 [==============================] - 35s 397ms/step - loss: 2.3412e-04 - mae: 0.0109 - lr: 1.0195e-04\n",
            "Epoch 240/500\n",
            "88/88 [==============================] - 31s 354ms/step - loss: 2.3057e-04 - mae: 0.0108 - lr: 1.0195e-04\n",
            "Epoch 241/500\n",
            "88/88 [==============================] - 33s 371ms/step - loss: 2.2797e-04 - mae: 0.0107 - lr: 1.0195e-04\n",
            "Epoch 242/500\n",
            "88/88 [==============================] - 31s 353ms/step - loss: 2.2647e-04 - mae: 0.0107 - lr: 1.0195e-04\n",
            "Epoch 243/500\n",
            "88/88 [==============================] - 32s 361ms/step - loss: 2.2890e-04 - mae: 0.0108 - lr: 9.6856e-05\n",
            "Epoch 244/500\n",
            "88/88 [==============================] - 32s 365ms/step - loss: 2.2071e-04 - mae: 0.0106 - lr: 9.6856e-05\n",
            "Epoch 245/500\n",
            "88/88 [==============================] - 34s 385ms/step - loss: 2.1598e-04 - mae: 0.0104 - lr: 9.6856e-05\n",
            "Epoch 246/500\n",
            "88/88 [==============================] - 29s 335ms/step - loss: 2.1291e-04 - mae: 0.0104 - lr: 9.6856e-05\n",
            "Epoch 247/500\n",
            "88/88 [==============================] - 33s 372ms/step - loss: 2.1187e-04 - mae: 0.0103 - lr: 9.6856e-05\n",
            "Epoch 248/500\n",
            "88/88 [==============================] - 36s 416ms/step - loss: 2.1510e-04 - mae: 0.0104 - lr: 9.2013e-05\n",
            "Epoch 249/500\n",
            "88/88 [==============================] - 30s 341ms/step - loss: 2.0962e-04 - mae: 0.0103 - lr: 9.2013e-05\n",
            "Epoch 250/500\n",
            "88/88 [==============================] - 32s 371ms/step - loss: 2.0591e-04 - mae: 0.0102 - lr: 9.2013e-05\n",
            "Epoch 251/500\n",
            "88/88 [==============================] - 34s 385ms/step - loss: 2.0434e-04 - mae: 0.0101 - lr: 9.2013e-05\n",
            "Epoch 252/500\n",
            "88/88 [==============================] - 32s 363ms/step - loss: 2.0375e-04 - mae: 0.0101 - lr: 9.2013e-05\n",
            "Epoch 253/500\n",
            "88/88 [==============================] - 34s 381ms/step - loss: 2.0674e-04 - mae: 0.0102 - lr: 8.7412e-05\n",
            "Epoch 254/500\n",
            "88/88 [==============================] - 35s 403ms/step - loss: 2.0192e-04 - mae: 0.0100 - lr: 8.7412e-05\n",
            "Epoch 255/500\n",
            "88/88 [==============================] - 41s 469ms/step - loss: 1.9885e-04 - mae: 0.0100 - lr: 8.7412e-05\n",
            "Epoch 256/500\n",
            "88/88 [==============================] - 44s 505ms/step - loss: 1.9793e-04 - mae: 0.0099 - lr: 8.7412e-05\n",
            "Epoch 257/500\n",
            "88/88 [==============================] - 42s 479ms/step - loss: 1.9791e-04 - mae: 0.0099 - lr: 8.7412e-05\n",
            "Epoch 258/500\n",
            "88/88 [==============================] - 41s 465ms/step - loss: 2.0144e-04 - mae: 0.0100 - lr: 8.3042e-05\n",
            "Epoch 259/500\n",
            "88/88 [==============================] - 40s 447ms/step - loss: 1.9776e-04 - mae: 0.0099 - lr: 8.3042e-05\n",
            "Epoch 260/500\n",
            "88/88 [==============================] - 52s 590ms/step - loss: 1.9510e-04 - mae: 0.0099 - lr: 8.3042e-05\n",
            "Epoch 261/500\n",
            "88/88 [==============================] - 41s 467ms/step - loss: 1.9438e-04 - mae: 0.0098 - lr: 8.3042e-05\n",
            "Epoch 262/500\n",
            "88/88 [==============================] - 38s 436ms/step - loss: 1.9474e-04 - mae: 0.0099 - lr: 8.3042e-05\n",
            "Epoch 263/500\n",
            "88/88 [==============================] - 39s 445ms/step - loss: 1.9793e-04 - mae: 0.0099 - lr: 7.8890e-05\n",
            "Epoch 264/500\n",
            "88/88 [==============================] - 32s 366ms/step - loss: 1.9344e-04 - mae: 0.0098 - lr: 7.8890e-05\n",
            "Epoch 265/500\n",
            "88/88 [==============================] - 36s 408ms/step - loss: 1.9151e-04 - mae: 0.0097 - lr: 7.8890e-05\n",
            "Epoch 266/500\n",
            "88/88 [==============================] - 39s 449ms/step - loss: 1.8997e-04 - mae: 0.0097 - lr: 7.8890e-05\n",
            "Epoch 267/500\n",
            "88/88 [==============================] - 35s 403ms/step - loss: 1.8939e-04 - mae: 0.0097 - lr: 7.8890e-05\n",
            "Epoch 268/500\n",
            "88/88 [==============================] - 34s 386ms/step - loss: 1.9200e-04 - mae: 0.0098 - lr: 7.4945e-05\n",
            "Epoch 269/500\n",
            "88/88 [==============================] - 35s 390ms/step - loss: 1.8726e-04 - mae: 0.0096 - lr: 7.4945e-05\n",
            "Epoch 270/500\n",
            "88/88 [==============================] - 36s 408ms/step - loss: 1.8556e-04 - mae: 0.0096 - lr: 7.4945e-05\n",
            "Epoch 271/500\n",
            "88/88 [==============================] - 33s 371ms/step - loss: 1.8486e-04 - mae: 0.0096 - lr: 7.4945e-05\n",
            "Epoch 272/500\n",
            "88/88 [==============================] - 34s 383ms/step - loss: 1.8510e-04 - mae: 0.0096 - lr: 7.4945e-05\n",
            "Epoch 273/500\n",
            "88/88 [==============================] - 36s 403ms/step - loss: 1.8761e-04 - mae: 0.0096 - lr: 7.1198e-05\n",
            "Epoch 274/500\n",
            "88/88 [==============================] - 32s 368ms/step - loss: 1.8489e-04 - mae: 0.0096 - lr: 7.1198e-05\n",
            "Epoch 275/500\n",
            "88/88 [==============================] - 35s 399ms/step - loss: 1.8360e-04 - mae: 0.0095 - lr: 7.1198e-05\n",
            "Epoch 276/500\n",
            "88/88 [==============================] - 34s 384ms/step - loss: 1.8267e-04 - mae: 0.0095 - lr: 7.1198e-05\n",
            "Epoch 277/500\n",
            "88/88 [==============================] - 30s 345ms/step - loss: 1.8226e-04 - mae: 0.0095 - lr: 7.1198e-05\n",
            "Epoch 278/500\n",
            "88/88 [==============================] - 38s 431ms/step - loss: 1.8427e-04 - mae: 0.0095 - lr: 6.7638e-05\n",
            "Epoch 279/500\n",
            "88/88 [==============================] - 32s 362ms/step - loss: 1.8097e-04 - mae: 0.0094 - lr: 6.7638e-05\n",
            "Epoch 280/500\n",
            "88/88 [==============================] - 33s 380ms/step - loss: 1.7925e-04 - mae: 0.0094 - lr: 6.7638e-05\n",
            "Epoch 281/500\n",
            "88/88 [==============================] - 36s 399ms/step - loss: 1.7801e-04 - mae: 0.0093 - lr: 6.7638e-05\n",
            "Epoch 282/500\n",
            "88/88 [==============================] - 31s 352ms/step - loss: 1.7807e-04 - mae: 0.0093 - lr: 6.7638e-05\n",
            "Epoch 283/500\n",
            "88/88 [==============================] - 35s 396ms/step - loss: 1.7983e-04 - mae: 0.0094 - lr: 6.4256e-05\n",
            "Epoch 284/500\n",
            "88/88 [==============================] - 32s 366ms/step - loss: 1.7673e-04 - mae: 0.0093 - lr: 6.4256e-05\n",
            "Epoch 285/500\n",
            "88/88 [==============================] - 34s 385ms/step - loss: 1.7542e-04 - mae: 0.0093 - lr: 6.4256e-05\n",
            "Epoch 286/500\n",
            "88/88 [==============================] - 36s 412ms/step - loss: 1.7479e-04 - mae: 0.0092 - lr: 6.4256e-05\n",
            "Epoch 287/500\n",
            "88/88 [==============================] - 37s 412ms/step - loss: 1.7441e-04 - mae: 0.0092 - lr: 6.4256e-05\n",
            "Epoch 288/500\n",
            "88/88 [==============================] - 36s 406ms/step - loss: 1.7632e-04 - mae: 0.0093 - lr: 6.1043e-05\n",
            "Epoch 289/500\n",
            "88/88 [==============================] - 34s 385ms/step - loss: 1.7373e-04 - mae: 0.0092 - lr: 6.1043e-05\n",
            "Epoch 290/500\n",
            "88/88 [==============================] - 37s 419ms/step - loss: 1.7240e-04 - mae: 0.0092 - lr: 6.1043e-05\n",
            "Epoch 291/500\n",
            "88/88 [==============================] - 34s 378ms/step - loss: 1.7200e-04 - mae: 0.0092 - lr: 6.1043e-05\n",
            "Epoch 292/500\n",
            "88/88 [==============================] - 34s 385ms/step - loss: 1.7225e-04 - mae: 0.0092 - lr: 6.1043e-05\n",
            "Epoch 293/500\n",
            "88/88 [==============================] - 34s 391ms/step - loss: 1.7367e-04 - mae: 0.0092 - lr: 5.7991e-05\n",
            "Epoch 294/500\n",
            "88/88 [==============================] - 36s 414ms/step - loss: 1.7136e-04 - mae: 0.0091 - lr: 5.7991e-05\n",
            "Epoch 295/500\n",
            "88/88 [==============================] - 37s 417ms/step - loss: 1.6990e-04 - mae: 0.0091 - lr: 5.7991e-05\n",
            "Epoch 296/500\n",
            "88/88 [==============================] - 33s 378ms/step - loss: 1.6965e-04 - mae: 0.0091 - lr: 5.7991e-05\n",
            "Epoch 297/500\n",
            "88/88 [==============================] - 36s 410ms/step - loss: 1.6930e-04 - mae: 0.0091 - lr: 5.7991e-05\n",
            "Epoch 298/500\n",
            "88/88 [==============================] - 37s 419ms/step - loss: 1.7096e-04 - mae: 0.0091 - lr: 5.5092e-05\n",
            "Epoch 299/500\n",
            "88/88 [==============================] - 33s 379ms/step - loss: 1.6864e-04 - mae: 0.0090 - lr: 5.5092e-05\n",
            "Epoch 300/500\n",
            "88/88 [==============================] - 33s 372ms/step - loss: 1.6738e-04 - mae: 0.0090 - lr: 5.5092e-05\n",
            "Epoch 301/500\n",
            "88/88 [==============================] - 35s 399ms/step - loss: 1.6660e-04 - mae: 0.0090 - lr: 5.5092e-05\n",
            "Epoch 302/500\n",
            "88/88 [==============================] - 33s 383ms/step - loss: 1.6686e-04 - mae: 0.0090 - lr: 5.5092e-05\n",
            "Epoch 303/500\n",
            "88/88 [==============================] - 38s 435ms/step - loss: 1.6802e-04 - mae: 0.0090 - lr: 5.2337e-05\n",
            "Epoch 304/500\n",
            "88/88 [==============================] - 35s 406ms/step - loss: 1.6579e-04 - mae: 0.0090 - lr: 5.2337e-05\n",
            "Epoch 305/500\n",
            "88/88 [==============================] - 33s 379ms/step - loss: 1.6468e-04 - mae: 0.0089 - lr: 5.2337e-05\n",
            "Epoch 306/500\n",
            "88/88 [==============================] - 35s 398ms/step - loss: 1.6401e-04 - mae: 0.0089 - lr: 5.2337e-05\n",
            "Epoch 307/500\n",
            "88/88 [==============================] - 35s 395ms/step - loss: 1.6459e-04 - mae: 0.0089 - lr: 5.2337e-05\n",
            "Epoch 308/500\n",
            "88/88 [==============================] - 34s 384ms/step - loss: 1.6598e-04 - mae: 0.0090 - lr: 4.9720e-05\n",
            "Epoch 309/500\n",
            "88/88 [==============================] - 40s 458ms/step - loss: 1.6419e-04 - mae: 0.0089 - lr: 4.9720e-05\n",
            "Epoch 310/500\n",
            "88/88 [==============================] - 31s 357ms/step - loss: 1.6315e-04 - mae: 0.0089 - lr: 4.9720e-05\n",
            "Epoch 311/500\n",
            "88/88 [==============================] - 33s 367ms/step - loss: 1.6280e-04 - mae: 0.0089 - lr: 4.9720e-05\n",
            "Epoch 312/500\n",
            "88/88 [==============================] - 35s 393ms/step - loss: 1.6278e-04 - mae: 0.0089 - lr: 4.9720e-05\n",
            "Epoch 313/500\n",
            "88/88 [==============================] - 34s 390ms/step - loss: 1.6314e-04 - mae: 0.0089 - lr: 4.9720e-05\n",
            "Epoch 314/500\n",
            "88/88 [==============================] - 30s 348ms/step - loss: 1.6368e-04 - mae: 0.0089 - lr: 4.9720e-05\n",
            "Epoch 315/500\n",
            "88/88 [==============================] - 37s 419ms/step - loss: 1.6463e-04 - mae: 0.0089 - lr: 4.9720e-05\n",
            "Epoch 316/500\n",
            "88/88 [==============================] - 35s 397ms/step - loss: 1.6494e-04 - mae: 0.0089 - lr: 4.9720e-05\n",
            "Epoch 317/500\n",
            "88/88 [==============================] - 34s 386ms/step - loss: 1.6534e-04 - mae: 0.0089 - lr: 4.9720e-05\n",
            "Epoch 318/500\n",
            "88/88 [==============================] - 31s 350ms/step - loss: 1.6640e-04 - mae: 0.0090 - lr: 4.7234e-05\n",
            "Epoch 319/500\n",
            "88/88 [==============================] - 39s 436ms/step - loss: 1.6450e-04 - mae: 0.0089 - lr: 4.7234e-05\n",
            "Epoch 320/500\n",
            "88/88 [==============================] - 25s 280ms/step - loss: 1.6329e-04 - mae: 0.0089 - lr: 4.7234e-05\n",
            "Epoch 321/500\n",
            "37/88 [===========>..................] - ETA: 9s - loss: 2.2741e-04 - mae: 0.0109"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(capsnet_lstm.summary())"
      ],
      "metadata": {
        "id": "bRU73bByvkyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69lcH5cAEnki"
      },
      "source": [
        "## Evaluating the forecast\n",
        "\n",
        "Now it is time to evaluate the performance of the forecast. For this we use the `compute_metrics` function:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7V0kTKHEnki"
      },
      "source": [
        "At this point only the model that will perform the forecast is ready but we still need to compute the actual forecast.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJtDyAD9Enkj"
      },
      "source": [
        "### Faster model forecasts\n",
        "\n",
        "\n",
        "- The dataset is windowed using `window_size` rather than `window_size + 1`\n",
        "- No shuffle should be used\n",
        "- No need to split the data into features and labels\n",
        "- A model is used to predict batches of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIyFlTEwEnkj"
      },
      "source": [
        "Now compute the actual forecast:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the forecast for all the series\n",
        "capsnet_lstm_forecast = model_forecast(capsnet_lstm, series_norm, G.WINDOW_SIZE).squeeze()\n",
        "\n",
        "# Slice the forecast to get only the predictions for the test set\n",
        "series_test_hat_capsnet_lstm = capsnet_lstm_forecast[-len(time_test):]\n",
        "\n",
        "# Reverse normalization\n",
        "series_test_hat_capsnet_lstm = reverse_normalization(series_test_hat_capsnet_lstm)"
      ],
      "metadata": {
        "id": "k2HUvtSTAY1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the forecast\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test)\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test_hat_capsnet_lstm)\n",
        "plt.ylabel(\"USD\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "legend_drawn_flag = True\n",
        "plt.legend([\"Original Value (Gold)\", \"Predicted Value (Gold)\"], loc=0, frameon=legend_drawn_flag)\n",
        "plt.savefig(\"/content/gdrive/MyDrive/Forecasting_results/capsnet_lstm_forecasts_GOLD.svg\")"
      ],
      "metadata": {
        "id": "SmSKga7VAAKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmelEr6a3I70"
      },
      "outputs": [],
      "source": [
        "rmse_capsnet_lstm, mae_capsnet_lstm, mape_capsnet_lstm= compute_metrics(series_test, series_test_hat_capsnet_lstm)\n",
        "\n",
        "print(f\"rmse: {rmse_capsnet_lstm:.2f}, mae: {mae_capsnet_lstm:.2f} , mape: {mape_capsnet_lstm:.2f} for forecast\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smape_capsnet_lstm = smape(series_test, series_test_hat_capsnet_lstm)\n",
        "print(f\"smape: {smape_capsnet_lstm:.2f} for forecast\")"
      ],
      "metadata": {
        "id": "TxkN6Lw8fJPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "capsnet_lstm.save_weights('/content/gdrive/MyDrive/Saved_models/apsnet_lstm_Gold_weights.h5', overwrite=True, save_format=None, options=None)"
      ],
      "metadata": {
        "id": "D-RP0rcdVj3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline (LSTM, CNN-LSTM)"
      ],
      "metadata": {
        "id": "og2vNXLVgOeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM"
      ],
      "metadata": {
        "id": "DTXrC28w_ZGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with the best hp.\n",
        "def LSTM():\n",
        "   \n",
        "    model = tf.keras.models.Sequential([\n",
        "          tf.keras.layers.LSTM(100, input_shape=[G.WINDOW_SIZE, 1]),\n",
        "          tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    learning_rate = 0.0005\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        metrics=[\"mae\"])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "ay7_NsM8fmKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding Epoch number"
      ],
      "metadata": {
        "id": "n298uO7ngm_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of epoch is 500 for LSTM"
      ],
      "metadata": {
        "id": "Jc5BRZZHhfUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with the train dataset.\n",
        "with strategy.scope():\n",
        "  lstm = LSTM()\n",
        "# Fit with the entire dataset.\n",
        "history_lstm_train = lstm.fit(train_set, epochs=500,callbacks = [reduce_lr])"
      ],
      "metadata": {
        "id": "VveQt5UllpUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(lstm.summary())"
      ],
      "metadata": {
        "id": "CUVX9mrUwRc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the forecast for all the series\n",
        "lstm_forecast = model_forecast(lstm, series_norm, G.WINDOW_SIZE).squeeze()\n",
        "\n",
        "# Slice the forecast to get only the predictions for the test set\n",
        "series_test_hat_lstm = lstm_forecast[-len(time_test):]\n",
        "series_test_hat_lstm = reverse_normalization(series_test_hat_lstm)"
      ],
      "metadata": {
        "id": "usOQ3cl6_rPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the forecast\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test)\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test_hat_lstm)\n",
        "plt.ylabel(\"USD\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "legend_drawn_flag = True\n",
        "plt.legend([\"Original Value (Gold)\", \"Predicted Value (Gold)\"], loc=0, frameon=legend_drawn_flag)\n",
        "plt.savefig(\"/content/gdrive/MyDrive/Forecasting_results/lstm_forecasts_Gold.svg\")"
      ],
      "metadata": {
        "id": "B_fYj_UQJjl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_lstm, mae_lstm, mape_lstm= compute_metrics(series_test, series_test_hat_lstm)\n",
        "\n",
        "print(f\"rmse: {rmse_lstm:.2f}, mae: {mae_lstm:.2f} , mape: {mape_lstm:.2f} for forecast\")"
      ],
      "metadata": {
        "id": "-U4UwUsf_Tp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smape_lstm = smape(series_test, series_test_hat_lstm)\n",
        "print(f\"smape: {smape_lstm:.2f} for forecast\")"
      ],
      "metadata": {
        "id": "6QZJDUsRf9_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "lstm.save(\"/content/gdrive/MyDrive/Saved_models/lstm_Gold.h5\")"
      ],
      "metadata": {
        "id": "Rx-8-GmWqAbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN-LSTM"
      ],
      "metadata": {
        "id": "EgSv9-FX_UEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_LSTM():\n",
        "   \n",
        "    model = tf.keras.models.Sequential([\n",
        "          tf.keras.layers.Conv1D(filters=512, kernel_size=2,\n",
        "                                 strides=1,\n",
        "                                 activation=\"relu\",\n",
        "                                 padding='causal',\n",
        "                                 input_shape=[G.WINDOW_SIZE, 1]),\n",
        "          tf.keras.layers.MaxPooling1D(pool_size=2, strides=1, padding=\"same\"),\n",
        "          tf.keras.layers.LSTM(100),\n",
        "          tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    learning_rate = 0.0005\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        metrics=[\"mae\"])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Amka1E2csj3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Epoch number is 500."
      ],
      "metadata": {
        "id": "Y8uS607RIn-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with the train dataset.\n",
        "with strategy.scope():\n",
        "  cnn_lstm = CNN_LSTM()\n",
        "# Fit with the entire dataset.\n",
        "history_cnn_lstm_train = cnn_lstm.fit(train_set, epochs=500,callbacks = [reduce_lr])"
      ],
      "metadata": {
        "id": "TM5Zy7HdmTgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cnn_lstm.summary())"
      ],
      "metadata": {
        "id": "JRRKJ2VswVHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the forecast for all the series\n",
        "cnn_lstm_forecast = model_forecast(cnn_lstm, series_norm, G.WINDOW_SIZE).squeeze()\n",
        "\n",
        "# Slice the forecast to get only the predictions for the test set\n",
        "series_test_hat_cnn_lstm = cnn_lstm_forecast[-len(time_test):]\n",
        "series_test_hat_cnn_lstm = reverse_normalization(series_test_hat_cnn_lstm)"
      ],
      "metadata": {
        "id": "54nm9-qaAU9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the forecast\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test)\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test_hat_cnn_lstm)\n",
        "plt.ylabel(\"USD\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "legend_drawn_flag = True\n",
        "plt.legend([\"Original Value (Gold)\", \"Predicted Value (Gold)\"], loc=0, frameon=legend_drawn_flag)\n",
        "plt.savefig(\"/content/gdrive/MyDrive/Forecasting_results/cnn_lstm_forecasts_Gold.svg\")"
      ],
      "metadata": {
        "id": "aunNgSHlJ7A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_cnn_lstm, mae_cnn_lstm, mape_cnn_lstm= compute_metrics(series_test, series_test_hat_cnn_lstm)\n",
        "\n",
        "print(f\"rmse: {rmse_cnn_lstm:.2f}, mae: {mae_cnn_lstm:.2f} , mape: {mape_cnn_lstm:.2f} for forecast\")"
      ],
      "metadata": {
        "id": "hW0VZCxdAVjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smape_cnn_lstm = smape(series_test, series_test_hat_cnn_lstm)\n",
        "print(f\"smape: {smape_cnn_lstm:.2f} for forecast\")"
      ],
      "metadata": {
        "id": "jRA9afd3gz4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_lstm.save(\"/content/gdrive/MyDrive/Saved_models/cnn_lstm_Gold.h5\")"
      ],
      "metadata": {
        "id": "JWGGlYp9h3rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(30, 20))\n",
        "plt.rcParams['font.size'] = '16'\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test)\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test_hat_capsnet_lstm)\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test_hat_lstm)\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test_hat_cnn_lstm)\n",
        "plt.ylabel(\"USD\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "legend_drawn_flag = True\n",
        "plt.legend([\"Original Value (Gold)\", \"Predicted Value (CapsNet-LSTM)\",\"Predicted Value (LSTM)\",\"Predicted Value (CNN-LSTM)\"], loc=0, frameon=legend_drawn_flag)\n",
        "plt.savefig(\"/content/gdrive/MyDrive/Forecasting_results/all_together_forecasts_Gold.svg\")"
      ],
      "metadata": {
        "id": "HV9A-uEqheTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rmse_capsnet_lstm)\n",
        "print(rmse_lstm)\n",
        "print(rmse_cnn_lstm)"
      ],
      "metadata": {
        "id": "i04gcQHRRnFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mae_capsnet_lstm)\n",
        "print(mae_lstm)\n",
        "print(mae_cnn_lstm)"
      ],
      "metadata": {
        "id": "udvSlzwBRmU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mape_capsnet_lstm)\n",
        "print(mape_lstm)\n",
        "print(mape_cnn_lstm)"
      ],
      "metadata": {
        "id": "LGqMIsnTRlQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(smape_capsnet_lstm)\n",
        "print(smape_lstm)\n",
        "print(smape_cnn_lstm)"
      ],
      "metadata": {
        "id": "jMDy4xR7hq6X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}