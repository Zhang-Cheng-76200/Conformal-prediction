{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zhang-Cheng-76200/CapsNet-LSTM/blob/main/CapsNet_LSTM_Gold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G92ON7NXEnkW"
      },
      "source": [
        "# Gold Price Forecasting Using A Hybrid CapsNet-LSTM Architecture\n",
        "\n",
        "Gold dataset contains data of the daily Gold prices recorded from Oct 1st, 2007 to Sep 25th, 2020. In addition to be using Tensorflow's layers for processing sequence data such as LSTMs we will also intergrate Capsule Network in our proposed neural architecture to improve the model's performance.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "56XEQOGknrAk"
      },
      "outputs": [],
      "source": [
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras import initializers, layers, models, optimizers, callbacks, utils\n",
        "from keras.models import load_model\n",
        "from keras.models import model_from_json\n",
        "\n",
        "from keras.initializers import glorot_uniform\n",
        "\n",
        "from keras.initializers import *\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.optimizers import *\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import *\n",
        "\n",
        "from keras import losses\n",
        "from keras.utils.io_utils import ask_to_proceed_with_overwrite\n",
        "from tensorflow.python.platform import tf_logging as logging\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Xe-Rnq9QbQ7A"
      },
      "outputs": [],
      "source": [
        "# Set seed\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "tf.random.set_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnjuziNxZdvf"
      },
      "outputs": [],
      "source": [
        "# If code is running on TPU\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb5Za_rOMFVx"
      },
      "outputs": [],
      "source": [
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA-JsdF3y5eE"
      },
      "outputs": [],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBR7h1IgEnkc"
      },
      "source": [
        "## Parsing the raw data\n",
        "\n",
        "A couple of things to note:\n",
        "\n",
        "- There is no need to save the data points as numpy arrays, regular lists is fine.\n",
        "- The `time` list should contain every timestep (starting at zero), which is just a sequence of ordered numbers with the same length as the `series` list.\n",
        "- The values of the `series` should be of `float` type. You can use Python's built-in `float` function to ensure this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeP3JMZHEnkZ"
      },
      "source": [
        "Begin by looking at the structure of the xlsx file that contains the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4Vkj4Y8DYm7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount ('gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKv3toTHFgnh"
      },
      "outputs": [],
      "source": [
        "Gold_raw = pd.read_excel('/content/gdrive/MyDrive/Gold/Gold.xlsx')\n",
        "Gold_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTg-wdf-Vpbi"
      },
      "outputs": [],
      "source": [
        "series = Gold_raw.iloc[:,1] # price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P3cPCrbVpVl"
      },
      "outputs": [],
      "source": [
        "time = []\n",
        "for i in range(0, len(series)):\n",
        "  time.append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQh2IH8Tl0Le"
      },
      "source": [
        "A helper function to plot the time series:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMGyMzGelzEW"
      },
      "outputs": [],
      "source": [
        "def plot_series(time, series, format=\"-\", start=0, end=None):\n",
        "    plt.plot(time[start:end], series[start:end], format)\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Value\")\n",
        "    plt.grid(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT1tB8J1Enkd"
      },
      "source": [
        "The next cell will use functions to compute the `time` and `series` and will save these as numpy arrays within the `G` dataclass. This cell will also plot the time series:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8UYoZlKEnke"
      },
      "outputs": [],
      "source": [
        "# Save all \"global\" variables within the G class (G stands for global)\n",
        "@dataclass\n",
        "class G:\n",
        "    TIME = np.array(time)\n",
        "    SERIES = np.array(series)\n",
        "    SPLIT_TIME = int(len(series)*0.9)          # data splitting.\n",
        "    WINDOW_SIZE = 250                                # sequence length\n",
        "    BATCH_SIZE = 32                                  # batch size\n",
        "\n",
        "plt.rcParams['font.size'] = '16'\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(G.TIME, G.SERIES)\n",
        "plt.ylabel(\"USD\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "legend_drawn_flag = True\n",
        "plt.legend([\"Gold per ounce price\"], loc=0, frameon=legend_drawn_flag)\n",
        "plt.savefig(\"/content/gdrive/MyDrive/Gold/Gold_original.svg\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8U3ckcaEnke"
      },
      "source": [
        "## Processing the data\n",
        "\n",
        "The `train_val_split` and `windowed_dataset` functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O7A_9XdEnkf"
      },
      "outputs": [],
      "source": [
        "def train_test_split(time, series, time_step=G.SPLIT_TIME):\n",
        "\n",
        "    time_train = time[:time_step]\n",
        "    series_train = series[:time_step]\n",
        "    time_test = time[time_step:]\n",
        "    series_test = series[time_step:]\n",
        "   \n",
        "\n",
        "    return time_train, series_train, time_test, series_test\n",
        "\n",
        "# Split the dataset\n",
        "time_train, series_train, time_test, series_test = train_test_split(G.TIME, G.SERIES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8xZy3QTZD8e"
      },
      "source": [
        "min-max normalization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRkvaM6AauAy"
      },
      "outputs": [],
      "source": [
        "def min_max_normalization(series):\n",
        "  series = (series - min(series_train))/(max(series_train) - min(series_train))\n",
        "  return series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR8bYse7ZC-G"
      },
      "outputs": [],
      "source": [
        "series_norm = min_max_normalization(G.SERIES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQr8LSVw4BIV"
      },
      "outputs": [],
      "source": [
        "series_train_norm = series_norm[:G.SPLIT_TIME]\n",
        "series_test_norm = series_norm[G.SPLIT_TIME:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izPRK1HJS8Qh"
      },
      "source": [
        "Building train_set:\n",
        "the train_set is used for training of the optimized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uix6YCADP4wK"
      },
      "outputs": [],
      "source": [
        "# Create train_set, val_set, and train_val_set. train_set and val_set are used for hyper=parameters tuning, and train_val_set is used for training.\n",
        "def windowed_dataset(series, window_size=G.WINDOW_SIZE, batch_size=G.BATCH_SIZE):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.map(lambda w: (w[:-1], w[-1]))\n",
        "    ds = ds.batch(batch_size).prefetch(1)\n",
        "    return ds\n",
        "train_set = windowed_dataset(series_train_norm, window_size=G.WINDOW_SIZE, batch_size=G.BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6stIQIqEnkh"
      },
      "source": [
        "## Compiling the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaMon77MEnkf"
      },
      "source": [
        "### Defining the model architecture (CapsNet-LSTM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAPDHkls8FGx"
      },
      "outputs": [],
      "source": [
        "# Squash function\n",
        "\n",
        "def squash(vectors, axis=-1):\n",
        "    \"\"\"\n",
        "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        "    :param vectors: some vectors to be squashed, N-dim tensor\n",
        "    :param axis: the axis to squash\n",
        "    :return: a Tensor with same shape as input vectors\n",
        "    \"\"\"\n",
        "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True) \n",
        "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
        "    return scale * vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2SUkh9LAFqP"
      },
      "outputs": [],
      "source": [
        "# Dynamic routing (layer)\n",
        "#    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
        "#    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "#    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
        "#    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
        "\n",
        "#    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
        "#    :param routings: number of iterations for the routing algorithm \n",
        "\n",
        "#@tf.keras.utils.register_keras_serializable() should use this line next time when need to save the model with a custom layer.\n",
        "class Routing(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, dim_capsule = 32 , routings=3, kernel_initializer='glorot_uniform'):\n",
        "        super(Routing, self).__init__()\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # build tansform matrix which can convert one primary cap to a vector with the same order as the digit cap.\n",
        "        # assert len(input_shape) >= 3  # The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\n",
        "        self.input_num_capsule = input_shape[1]\n",
        "        self.input_dim_capsule = input_shape[2]\n",
        "        \n",
        "        # Transform matrix\n",
        "       \n",
        "        self.W = self.add_weight(shape=[self.input_num_capsule,self.dim_capsule, self.input_dim_capsule],initializer=self.kernel_initializer,name='W')\n",
        "\n",
        "    def call(self, inputs): #training=None\n",
        "        # inputs.shape=[input_num_capsule, input_dim_capsule]\n",
        "        # inputs_expand.shape=[input_num_capsule, input_dim_capsule, 1]\n",
        "        inputs_expand = K.expand_dims(inputs, -1)    \n",
        "\n",
        "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0.\n",
        "        # x.shape=[None, input_num_capsule, input_dim_capsule, 1]\n",
        "        # W.shape=[input_num_capsule, dim_capsule, input_dim_capsule]\n",
        "        # Regard the first dimension as `batch` dimension,\n",
        "        # then matmul: [dim_capsule, input_dim_capsule] x [input_dim_capsule, 1] -> [dim_capsule].\n",
        "        # inputs_hat.shape = [None, input_num_capsule, dim_capsule, 1]     \n",
        "          \n",
        "        inputs_hat = K.map_fn(lambda x: K.batch_dot(self.W, x, [2, 1]), elems=inputs_expand) \n",
        "\n",
        "        inputs_hat = K.squeeze(inputs_hat, axis = -1)\n",
        "        # inputs_hat.shape = [None, input_num_capsule, dim_capsule]\n",
        "       \n",
        "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        # b.shape = [None, self.input_num_capsule].\n",
        "        b = tf.zeros(shape=[K.shape(inputs_hat)[0], self.input_num_capsule])\n",
        "        \n",
        "        assert self.routings > 0\n",
        "        for i in range(self.routings):\n",
        "            # c.shape=[batch_size, input_num_capsule]\n",
        "            c = tf.nn.softmax(b, axis=-1)\n",
        "\n",
        "            # c.shape =  [batch_size, input_num_capsule]\n",
        "            # inputs_hat.shape=[batch_size, input_num_capsule, dim_capsule]\n",
        "            # The first dimensions as `batch` dimension,\n",
        "            # then matmal: [input_num_capsule] x [input_num_capsule, dim_capsule] -> [dim_capsule].\n",
        "            # outputs.shape=[None, dim_capsule]\n",
        "            \n",
        "            #outputs = squash(K.batch_dot(c, inputs_hat, [1, 1])) \n",
        "            outputs = K.batch_dot(c, inputs_hat, [1, 1])\n",
        "\n",
        "            if i < self.routings - 1:\n",
        "                # outputs.shape =  [None, dim_capsule]\n",
        "                # inputs_hat.shape=[None, input_num_capsule, dim_capsule]\n",
        "                # The first dimension as `batch` dimension,\n",
        "                # then matmal: [dim_capsule] x [input_num_capsule, dim_capsule]^T -> [input_num_capsule].\n",
        "                # b.shape=[batch_size, input_num_capsule]\n",
        "                b += K.batch_dot(outputs, inputs_hat, [1, 2])\n",
        "       \n",
        "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tuple([None, self.dim_capsule])\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"dim_capsule\": self.dim_capsule,\n",
        "            \"routings\": self.routings,\n",
        "            \"kernel_initializer\": self.kernel_initializer,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y12KaJYEnki"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(true_series, forecast):\n",
        "    \n",
        "    mse = tf.keras.metrics.mean_squared_error(true_series, forecast).numpy()   \n",
        "    rmse = tf.math.sqrt(mse).numpy()\n",
        "    mae = tf.keras.metrics.mean_absolute_error(true_series, forecast).numpy()\n",
        "    mape = tf.keras.metrics.mean_absolute_percentage_error(true_series, forecast).numpy()\n",
        "    return rmse, mae, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XwGrf-A_wF0"
      },
      "outputs": [],
      "source": [
        "def model_forecast(model, series, window_size):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "    ds = ds.batch(32).prefetch(1)\n",
        "    forecast = model.predict(ds)\n",
        "    return forecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xG8kJO3blWny"
      },
      "outputs": [],
      "source": [
        "def reverse_normalization(series):\n",
        "  series = series * (max(series_train) - min(series_train)) + min(series_train)\n",
        "  return series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh3dZo2eVk0K"
      },
      "outputs": [],
      "source": [
        "input_shape = (G.WINDOW_SIZE, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxTHiYBFhIhF"
      },
      "outputs": [],
      "source": [
        "# Prepare callback\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='mae', factor=0.95,patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dD6KzvejX_o"
      },
      "outputs": [],
      "source": [
        "K.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAgEm3Ue4PNF"
      },
      "source": [
        "### Determine the number of Epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqOfqDbaUhsm"
      },
      "outputs": [],
      "source": [
        "# Build the model with the default setting. LSTM units = 200, Convolution filters = 512, kernel size = 2, stride = 1, learning rate = 0.0005\n",
        "def CapsNet_LSTM():\n",
        "   \n",
        "    input = tf.keras.layers.Input(shape=input_shape)\n",
        "      \n",
        "    # First layer\n",
        "    conv1d_1 = tf.keras.layers.Conv1D(filters=512, kernel_size=2, strides=1, padding='causal', activation='relu', name = 'conv1d_1')(input)\n",
        "          \n",
        "    # Primary caps\n",
        "  \n",
        "    unsquashed_caps = tf.keras.layers.Reshape((G.WINDOW_SIZE, 64, 8))(conv1d_1)\n",
        "    squashed_caps = tf.keras.layers.Lambda(squash)(unsquashed_caps)\n",
        "\n",
        "    # Digit caps\n",
        "     \n",
        "    digit_caps = tf.keras.layers.TimeDistributed(Routing(dim_capsule = 512, routings = 3))(squashed_caps)\n",
        "\n",
        "    # LSTM layer\n",
        "    lstm = tf.keras.layers.LSTM(200)(digit_caps)\n",
        "\n",
        "    # Dense layer\n",
        "\n",
        "    output = tf.keras.layers.Dense(1)(lstm)\n",
        "\n",
        "    model = keras.Model(inputs=input, outputs=output, name=\"capsnet_lstm\")\n",
        "\n",
        "    learning_rate = 0.0005\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        metrics=[\"mae\"])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh0WNIt-XlNR"
      },
      "source": [
        "Finding epoch number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMTX52gu5Jmg"
      },
      "source": [
        "epoch number for CapsNet-LSTM is 500."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Bz4kTlU3C_r_"
      },
      "outputs": [],
      "source": [
        "# Train the model with the train dataset and val set combining together.\n",
        "with strategy.scope():\n",
        "  capsnet_lstm = CapsNet_LSTM()\n",
        "# Fit with the entire dataset.\n",
        "history_capsnet_lstm_train = capsnet_lstm.fit(train_set, epochs=500,callbacks = [reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRU73bByvkyi"
      },
      "outputs": [],
      "source": [
        "print(capsnet_lstm.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69lcH5cAEnki"
      },
      "source": [
        "## Evaluating the forecast\n",
        "\n",
        "Now it is time to evaluate the performance of the forecast. For this we use the `compute_metrics` function:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7V0kTKHEnki"
      },
      "source": [
        "At this point only the model that will perform the forecast is ready but we still need to compute the actual forecast.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJtDyAD9Enkj"
      },
      "source": [
        "### Faster model forecasts\n",
        "\n",
        "\n",
        "- The dataset is windowed using `window_size` rather than `window_size + 1`\n",
        "- No shuffle should be used\n",
        "- No need to split the data into features and labels\n",
        "- A model is used to predict batches of the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIyFlTEwEnkj"
      },
      "source": [
        "Now compute the actual forecast:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEiYgYaA28PR"
      },
      "outputs": [],
      "source": [
        "# Compute the forecast for all the series\n",
        "capsnet_lstm_forecast = model_forecast(capsnet_lstm, series_norm, G.WINDOW_SIZE).squeeze()\n",
        "\n",
        "# Slice the forecast to get only the predictions for the validation set\n",
        "series_test_hat_capsnet_lstm = capsnet_lstm_forecast[G.SPLIT_TIME:]\n",
        "\n",
        "# Reverse normalization\n",
        "series_test_hat_capsnet_lstm = reverse_normalization(series_test_hat_capsnet_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save test set and predicted values\n",
        "\n",
        "np.savetxt(\"/content/gdrive/MyDrive/Gold/test_set_values_Gold.csv\", \n",
        "           series_test.T, delimiter=\",\")\n",
        "\n",
        "np.savetxt(\"/content/gdrive/MyDrive/Gold/capsnet_lstm_predicted_values_Gold.csv\", \n",
        "           series_test_hat_capsnet_lstm.T, delimiter=\",\")"
      ],
      "metadata": {
        "id": "7qvo_SUgazfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmSKga7VAAKf"
      },
      "outputs": [],
      "source": [
        "# Plot the forecast\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test)\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test_hat_capsnet_lstm)\n",
        "plt.ylabel(\"USD\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "legend_drawn_flag = True\n",
        "plt.legend([\"Original Value (Gold)\", \"Predicted Value (Gold)\"], loc=0, frameon=legend_drawn_flag)\n",
        "plt.savefig(\"/content/gdrive/MyDrive/Gold/capsnet_lstm_forecasts_Gold.svg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmelEr6a3I70"
      },
      "outputs": [],
      "source": [
        "rmse_capsnet_lstm, mae_capsnet_lstm, mape_capsnet_lstm= compute_metrics(series_test, series_test_hat_capsnet_lstm)\n",
        "\n",
        "print(f\"rmse: {rmse_capsnet_lstm:.2f}, mae: {mae_capsnet_lstm:.2f} , mape: {mape_capsnet_lstm:.2f} for forecast\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-RP0rcdVj3p"
      },
      "outputs": [],
      "source": [
        "capsnet_lstm.save_weights('/content/gdrive/MyDrive/Gold/capsnet_lstm_Gold_weights.h5', overwrite=True, save_format=None, options=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og2vNXLVgOeC"
      },
      "source": [
        "## Baseline (LSTM, CNN-LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTXrC28w_ZGT"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay7_NsM8fmKT"
      },
      "outputs": [],
      "source": [
        "# Build the model with the best hp.\n",
        "def LSTM():\n",
        "   \n",
        "    model = tf.keras.models.Sequential([\n",
        "          tf.keras.layers.LSTM(200, input_shape=[G.WINDOW_SIZE, 1]),\n",
        "          tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    learning_rate = 0.0005\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        metrics=[\"mae\"])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n298uO7ngm_X"
      },
      "source": [
        "Finding Epoch number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc5BRZZHhfUi"
      },
      "source": [
        "The number of epoch is 500 for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VveQt5UllpUD"
      },
      "outputs": [],
      "source": [
        "# Train the model with the train dataset and val set combining together.\n",
        "with strategy.scope():\n",
        "  lstm = LSTM()\n",
        "# Fit with the entire dataset.\n",
        "history_lstm_train = lstm.fit(train_val_set, epochs=500,callbacks = [reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUVX9mrUwRc_"
      },
      "outputs": [],
      "source": [
        "print(lstm.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usOQ3cl6_rPV"
      },
      "outputs": [],
      "source": [
        "# Compute the forecast for all the series\n",
        "lstm_forecast = model_forecast(lstm, series_norm, G.WINDOW_SIZE).squeeze()\n",
        "\n",
        "# Slice the forecast to get only the predictions for the validation set\n",
        "series_test_hat_lstm = lstm_forecast[G.SPLIT_TIME:]\n",
        "series_test_hat_lstm = reverse_normalization(series_test_hat_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predicted values\n",
        "\n",
        "np.savetxt(\"/content/gdrive/MyDrive/Gold/lstm_predicted_values_Gold.csv\", \n",
        "           series_test_hat_lstm.T, delimiter=\",\")"
      ],
      "metadata": {
        "id": "VMpq5oJqbcDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_fYj_UQJjl1"
      },
      "outputs": [],
      "source": [
        "# Plot the forecast\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test)\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test_hat_lstm)\n",
        "plt.ylabel(\"USD\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "legend_drawn_flag = True\n",
        "plt.legend([\"Original Value (Gold)\", \"Predicted Value (Gold)\"], loc=0, frameon=legend_drawn_flag)\n",
        "plt.savefig(\"/content/gdrive/MyDrive/Gold/lstm_forecasts_Gold.svg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U4UwUsf_Tp2"
      },
      "outputs": [],
      "source": [
        "rmse_lstm, mae_lstm, mape_lstm= compute_metrics(series_test, series_test_hat_lstm)\n",
        "\n",
        "print(f\"rmse: {rmse_lstm:.2f}, mae: {mae_lstm:.2f} , mape: {mape_lstm:.2f} for forecast\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rx-8-GmWqAbz"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "lstm.save(\"/content/gdrive/MyDrive/Gold/lstm_Gold.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgSv9-FX_UEA"
      },
      "source": [
        "### CNN-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Amka1E2csj3H"
      },
      "outputs": [],
      "source": [
        "def CNN_LSTM():\n",
        "   \n",
        "    model = tf.keras.models.Sequential([\n",
        "          tf.keras.layers.Conv1D(filters=512, kernel_size=2,\n",
        "                                 strides=1,\n",
        "                                 activation=\"relu\",\n",
        "                                 padding='causal',\n",
        "                                 input_shape=[G.WINDOW_SIZE, 1]),\n",
        "          tf.keras.layers.MaxPooling1D(pool_size=2, strides=1, padding=\"same\"),\n",
        "          tf.keras.layers.LSTM(200),\n",
        "          tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "    learning_rate = 0.0005\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=tf.keras.losses.MeanSquaredError(),\n",
        "        metrics=[\"mae\"])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8uS607RIn-B"
      },
      "source": [
        "Epoch number is 500."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM5Zy7HdmTgU"
      },
      "outputs": [],
      "source": [
        "# Train the model with the train dataset and val set combining together.\n",
        "with strategy.scope():\n",
        "  cnn_lstm = CNN_LSTM()\n",
        "# Fit with the entire dataset.\n",
        "history_cnn_lstm_train = cnn_lstm.fit(train_set, epochs=500,callbacks = [reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRRKJ2VswVHY"
      },
      "outputs": [],
      "source": [
        "print(cnn_lstm.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54nm9-qaAU9o"
      },
      "outputs": [],
      "source": [
        "# Compute the forecast for all the series\n",
        "cnn_lstm_forecast = model_forecast(cnn_lstm, series_norm, G.WINDOW_SIZE).squeeze()\n",
        "\n",
        "# Slice the forecast to get only the predictions for the validation set\n",
        "series_test_hat_cnn_lstm = cnn_lstm_forecast[G.SPLIT_TIME:]\n",
        "series_test_hat_cnn_lstm = reverse_normalization(series_test_hat_cnn_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predicted values\n",
        "\n",
        "np.savetxt(\"/content/gdrive/MyDrive/Gold/cnn_lstm_predicted_values_Gold.csv\", \n",
        "           series_test_hat_lstm.T, delimiter=\",\")"
      ],
      "metadata": {
        "id": "OP4RkUCOblTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aunNgSHlJ7A6"
      },
      "outputs": [],
      "source": [
        "# Plot the forecast\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test)\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test_hat_cnn_lstm)\n",
        "plt.ylabel(\"USD\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "legend_drawn_flag = True\n",
        "plt.legend([\"Original Value (Gold)\", \"Predicted Value (Gold)\"], loc=0, frameon=legend_drawn_flag)\n",
        "plt.savefig(\"/content/gdrive/MyDrive/Gold/cnn_lstm_forecasts_Gold.svg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW0VZCxdAVjO"
      },
      "outputs": [],
      "source": [
        "rmse_cnn_lstm, mae_cnn_lstm, mape_cnn_lstm= compute_metrics(series_test, series_test_hat_cnn_lstm)\n",
        "\n",
        "print(f\"rmse: {rmse_cnn_lstm:.2f}, mae: {mae_cnn_lstm:.2f} , mape: {mape_cnn_lstm:.2f} for forecast\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWGGlYp9h3rG"
      },
      "outputs": [],
      "source": [
        "cnn_lstm.save(\"/content/gdrive/MyDrive/Gold/cnn_lstm_Gold.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV9A-uEqheTx"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(30, 20))\n",
        "plt.rcParams['font.size'] = '16'\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test)\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test_hat_capsnet_lstm)\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test_hat_lstm)\n",
        "plot_series(np.arange( 1, len(time_test)+1 ), series_test_hat_cnn_lstm)\n",
        "plt.ylabel(\"USD\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "legend_drawn_flag = True\n",
        "plt.legend([\"Original Value (Gold)\", \"Predicted Value (CapsNet-LSTM)\",\"Predicted Value (LSTM)\",\"Predicted Value (CNN-LSTM)\"], loc=0, frameon=legend_drawn_flag)\n",
        "plt.savefig(\"/content/gdrive/MyDrive/Gold/all_together_forecasts_Gold.svg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i04gcQHRRnFh"
      },
      "outputs": [],
      "source": [
        "print(rmse_capsnet_lstm)\n",
        "print(rmse_lstm)\n",
        "print(rmse_cnn_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udvSlzwBRmU7"
      },
      "outputs": [],
      "source": [
        "print(mae_capsnet_lstm)\n",
        "print(mae_lstm)\n",
        "print(mae_cnn_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGqMIsnTRlQL"
      },
      "outputs": [],
      "source": [
        "print(mape_capsnet_lstm)\n",
        "print(mape_lstm)\n",
        "print(mape_cnn_lstm)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}